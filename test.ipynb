{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Model\n",
    "\n",
    "Trying toy model inspired by [Griffiths & Steyvers, 2004](https://doi.org/10.1073/pnas.0307752101)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "tfd = tfp.distributions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Behaviour of `tfp.Distributions`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 0],\n",
       "       [2, 0],\n",
       "       [2, 0],\n",
       "       [2, 0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tfd.Categorical sampling behaviour:\n",
    "probs = tf.constant([[0., 0., 1.], \n",
    "                     [1., 0., 0.]])\n",
    "print(probs.shape)\n",
    "tfd.Categorical(probs=probs).sample(4).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02140409, 0.01427036, 0.9643255 ],\n",
       "       [0.38862276, 0.16176145, 0.44961584]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tfd.Dirichlet sampling behavious:\n",
    "conc = tf.constant([[0.1, 0.1, 0.1], \n",
    "                    [  2.,  2.,  2.]])\n",
    "tfd.Dirichlet(conc).sample().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ Samples come in by row. For Categorical the sample size is flexible. For Dirichlet the sample cise is bounded to the concentrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting up Topics and Data**\n",
    "\n",
    "1. Specify global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of Topics\n",
    "K = 10\n",
    "\n",
    "## Square Root of the Number of \"Vocabulary\" (must be sqrt such that pictorial interpretation is possible)\n",
    "sqrtV = 5\n",
    "\n",
    "## Number of words per document\n",
    "N = 100\n",
    "\n",
    "## Number of documents\n",
    "D = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define Topic-Word relations and priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word grid\n",
    "V = int(sqrtV**2)\n",
    "V_grid = np.reshape(np.arange(0, V), newshape=(sqrtV, sqrtV))\n",
    "\n",
    "## Topic-Word Distribution\n",
    "#  Words belonging to a topic are rows and columns\n",
    "Theta_idx = [row for row in V_grid] + [col for col in V_grid.T]\n",
    "Theta = np.zeros((K, V))\n",
    "for k, idx in enumerate(Theta_idx):\n",
    "    Theta[k, idx] = 1. / sqrtV\n",
    "Theta = tf.constant(Theta, dtype=tf.float32)\n",
    "\n",
    "## Document topic prior\n",
    "Alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000, 100])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Topic-Word Distibution is omitted because Theta is fixed\n",
    "\n",
    "## Document-Topic Distribution\n",
    "dist_Pi = tfd.Dirichlet(K*[Alpha])\n",
    "Pi      = dist_Pi.sample(D)\n",
    "\n",
    "## Topic Assignments of word c_{dik} of word w_{di}\n",
    "dist_C    = tfd.Categorical(probs=Pi)\n",
    "C         = tf.reshape(dist_C.sample(N), shape=(D, -1)) ## Its more efficient to reshape before converting to one_hot vectors\n",
    "C_one_hot = tf.one_hot(C, depth=K, axis=-1)\n",
    "assert tf.reduce_all(tf.reduce_sum(C_one_hot, axis=-1) == 1)\n",
    "\n",
    "## Draw words w_{di}\n",
    "dist_W = tfd.Categorical(tf.gather(Theta, C))\n",
    "W = dist_W.sample()\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000, 100, 25])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(Theta, C).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n",
      "(1000, 100)\n",
      "(1000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(Pi.shape)\n",
    "print(C.shape)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_to_img(arr, sqrtV=5):\n",
    "    vals = dict(zip(*np.unique(arr, return_counts=True)))\n",
    "    img = []\n",
    "\n",
    "    for i in range(V):\n",
    "        if i in vals:\n",
    "            img.append(vals[i])\n",
    "        else:\n",
    "            img.append(0)\n",
    "\n",
    "    img = np.array(img).reshape(sqrtV, sqrtV)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ace0faea70>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(format_to_img(W[100]), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Tensor\n",
    "One efficiency crtitical step is to vectorize \n",
    "$$\n",
    "    n_{dkv} =  \\{i \\, \\vert \\, w_{di} == v \\ \\& \\ c_{idk} ==1\\}\n",
    "$$\n",
    "which can be realized by stacking $W$ $K$ times along the last axis to match the shape of $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_N_tensor = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_tensor(W, C_one_hot, V=25):\n",
    "    W = W.numpy()\n",
    "    C_one_hot = C_one_hot.numpy()\n",
    "    n = np.zeros(shape=(C_one_hot.shape[0], C_one_hot.shape[2], V))\n",
    "    for v in range(V):\n",
    "        # print(v)\n",
    "        for i in range(C_one_hot.shape[1]):\n",
    "            for d in range(C_one_hot.shape[0]):\n",
    "                for k in range(C_one_hot.shape[2]):\n",
    "                    if W[d, i] == v and C_one_hot[d, i, k] == 1:\n",
    "                        n[d, k, v] += 1\n",
    "    return n\n",
    "\n",
    "@tf.function\n",
    "def tf_N_tensor(W, C_one_hot, V=25):\n",
    "\n",
    "    ## Extracting shapes\n",
    "    D = C_one_hot.shape[0]\n",
    "    N = C_one_hot.shape[1]\n",
    "    K = C_one_hot.shape[2]\n",
    "    \n",
    "    ## Preparing W-stacking by shifting all entries one \"up\" s. t. v is counted \n",
    "    #  from 1 to 25 instead from 0 to 24. This enables to collapse the \"&\" in the\n",
    "    #  set to be collapsed to a matrix product\n",
    "    Wp1 = W + 1\n",
    "    W_stacked = tf.stack(K*[Wp1], axis=-1)    \n",
    "\n",
    "    ## Elementwise product combines logical & in condition.\n",
    "    #  Choosing int32 as product dtype for efficiency.\n",
    "    C_one_hot_int = tf.cast(C_one_hot, dtype=tf.int32)\n",
    "    C_Dot_W = tf.math.multiply(W_stacked, C_one_hot_int)\n",
    "\n",
    "    ## The v-dimension of N is a one-hot encoding for the vocabulary:\n",
    "    N_DNKVp1 = tf.one_hot(C_Dot_W, 25+1, dtype=tf.int32)\n",
    "\n",
    "    ## Reverting the v-shift by dropping the 0 one-hot dimension\n",
    "    N_DNKV = N_DNKVp1[:, :, :, 1:]\n",
    "\n",
    "    ## Summing along v-dimension\n",
    "    assert N_DNKV.shape[1] == N\n",
    "    N_DKV = tf.reduce_sum(N_DNKV, axis=1)\n",
    "\n",
    "    ## Turn to float for gibbs sampler\n",
    "    N_DKV = tf.cast(N_DKV, dtype=tf.float32)\n",
    "\n",
    "    return N_DKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_N_tensor:\n",
    "    test1 = tf_N_tensor(tf.Variable(W), tf.Variable(C_one_hot))\n",
    "    test2 = n_tensor(W, C_one_hot)\n",
    "    assert list(test2.shape) == test1.shape.as_list()\n",
    "    assert np.all(test1.numpy() == test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling $C$\n",
    "\n",
    "Sampling $\\Theta$ and $\\Pi$ is not very problematic because they are just dirichlet distributed (below). Sampling $C$ is sampling from \n",
    "$$\n",
    "    p(C\\vert \\Theta, \\Pi, W)=\\prod_{d=1}^D \\prod_{i=1}^N \\frac{\\prod_{k=1}^K \\left(\\pi_{dk}\\theta_{kw_{di}}\\right)^{c_{dik}}}{\\sum_{k'=1}^K\\left(\\pi_{dk'}\\theta_{k'w_{di}}\\right)}\n",
    "$$ \n",
    "which is a categorical distribution.\n",
    "\n",
    "https://youtu.be/z2q7LhsnWNg?t=3878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n",
      "(10, 25)\n",
      "(1000, 100, 10)\n"
     ]
    }
   ],
   "source": [
    "print(Pi.shape)\n",
    "print(Theta.shape)\n",
    "print(C_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_C(Theta, Pi, W):\n",
    "\n",
    "    ## Numerator\n",
    "    Theta_block = tf.cast(tf.stack(D*[Theta]), dtype=tf.float32)\n",
    "    Pi_block = tf.cast(tf.stack(V*[Pi], axis=-1), dtype=tf.float32)\n",
    "    numerator = tf.math.multiply(Pi_block, Theta_block)\n",
    "\n",
    "    ## Denominator\n",
    "    denominator = tf.reduce_sum(numerator, axis=1)\n",
    "\n",
    "    C_dist = tfd.Categorical()\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1000, 100), dtype=int32, numpy=\n",
       "array([[12, 22,  3, ..., 17, 13, 23],\n",
       "       [13, 18,  9, ...,  3, 12, 10],\n",
       "       [ 1,  0, 19, ...,  6,  3,  5],\n",
       "       ...,\n",
       "       [13,  8, 15, ..., 21, 13, 16],\n",
       "       [15, 19,  6, ..., 22, 18, 12],\n",
       "       [ 4, 16, 21, ..., 22,  4,  6]])>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 25])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000, 100)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta.numpy()[:, W].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Here you go\n",
    "\n",
    "tf.gather(tf.transpose(Theta), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(tf.reshape(tf.gather(tf.transpose(Theta), W), (10, 1000, 100)), Theta.numpy()[:, W])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([102, 435, 860, 270, 106,  71, 700,  20, 614, 121, 466, 214, 330,\n       458,  87, 372,  99, 871, 663, 130, 661, 308, 769, 343, 491, 413,\n       805, 385, 191, 955, 276, 160, 459, 313,  21, 252, 747, 856, 560,\n       474,  58, 510, 681, 475, 699, 975, 782, 189, 957, 686, 957, 562,\n       875, 566, 243, 831, 504, 130, 484, 818, 646,  20, 840, 166, 273,\n       387, 600, 315,  13, 241, 776, 345, 564, 897, 339,  91, 366, 955,\n       454, 427, 508, 775, 942,  34, 205,  80, 931, 561, 871, 387,   1,\n       389, 565, 105, 771, 821, 476, 702, 401, 729, 555, 161, 201, 957,\n       995, 269, 862, 815, 270, 455, 461, 726, 251, 701, 295, 724, 719,\n       748, 337, 878,  52, 791, 921, 216, 763, 187, 379, 492,  40, 156,\n        14, 812,  64, 856, 838, 520, 343, 128, 647, 471,  62, 138, 498,\n       592, 391, 674, 418, 288, 378, 772, 489, 230,  40,  27, 134, 200,\n       839, 779, 929,  32,  47, 502, 406, 573, 727, 804,  98, 683, 871,\n       725, 986, 546, 960, 738, 612, 942, 461, 642, 768,   4, 217, 502,\n       766, 397, 870, 794, 392, 206,  14, 857, 553, 891, 460, 690, 574,\n       863, 742, 240, 563,  95, 899, 733, 484, 406, 230, 748, 654, 170,\n       540,  35, 524, 159, 838, 698, 242,  85, 795, 577, 681, 556, 573,\n       952, 645, 795,  27, 619, 555, 339, 797, 957, 330, 639, 505, 347,\n       472, 230, 189, 224, 384, 376, 282, 957, 632, 627, 972, 744, 258,\n       358, 709, 455, 410, 648, 317, 676, 224, 818, 233, 683, 663, 974,\n       826, 373, 671, 607, 471, 232, 691, 112, 829, 496, 441, 563, 267,\n       509, 806, 385, 386, 112, 612, 624, 951,  80, 698, 112,   1, 641,\n       219, 565, 854, 996, 735, 224, 384, 402, 637, 129,  52, 683, 729,\n       671, 709, 415, 246, 835, 438, 202, 183, 122, 400, 766, 293, 279,\n       836, 883, 609, 197, 981, 906, 510, 751, 143, 608, 200, 123, 186,\n       325, 463, 348, 770, 659, 763, 954, 931, 402, 345, 962, 510, 146,\n       147, 863, 710, 819, 488, 928, 935, 639, 550, 337, 871, 640, 778,\n       987, 952, 472, 945, 150, 414, 989, 297, 610, 262, 763, 143, 345,\n       623, 571, 880,   1, 896, 303, 253, 651, 452,  36, 159,   8, 232,\n        98, 658, 815, 207, 130, 403, 151,  53, 119, 672, 919, 627, 586,\n       624, 967, 419, 421, 103, 851, 253, 226, 111, 509, 472,  98, 152,\n       860, 913, 895, 877, 337, 705, 821, 162, 719, 956, 680, 995, 160,\n       579, 800, 397, 276, 815, 915, 503, 895, 391, 134, 194, 400, 639,\n        32, 687, 459, 954, 882, 469, 374,  21, 749, 669,  37, 229, 364,\n       562, 437, 775, 282,  26, 225, 276, 797, 608, 283, 878, 959, 480,\n       452, 828, 815, 658, 515, 546, 191,  48, 511,  16, 171, 219, 157,\n       476,  45, 372, 517,  98, 891, 744,  36, 279, 348, 496, 301, 180,\n       606,  98, 699, 992, 115, 190, 252, 980, 927, 982, 160, 255, 322,\n       127,  17, 792, 734, 565, 569, 322, 871, 685, 791, 625, 287, 942,\n       853, 662, 961, 638, 154, 489, 385, 985, 784, 103, 928, 392, 810,\n       245, 175,  38, 476, 681, 758, 537, 866, 817, 920, 407, 524, 827,\n       505, 902, 824,  35, 684,  19, 320, 775, 511, 399, 653, 971, 882,\n       470, 142,  91, 353, 833, 799, 726, 958, 853,  50, 664, 697, 574,\n       189, 124, 149, 313, 569, 341, 304, 691, 681, 837, 782,  53, 443,\n       612, 992, 263,  52, 571, 619,   4, 102, 195, 773, 876, 991, 883,\n       349,  46, 866, 822, 935, 819, 655, 268, 369, 635, 105, 669, 658,\n       656, 119, 830, 786, 603,  57, 950, 345, 740, 473, 116, 829, 790,\n       126, 392, 907, 640,  57, 633, 512, 750, 801,  95, 637, 117, 559,\n       600, 487, 236, 884, 896, 271, 188, 998, 703, 446, 580, 789, 860,\n       246, 962,  75, 153, 655, 434, 996,  85, 696, 284, 973, 219,  68,\n        46,  93, 749, 957, 452, 203, 911, 217, 473, 431, 340, 550, 611,\n       288, 253, 733, 356,  22, 761, 521, 757, 836,  99, 801, 179, 222,\n       905, 761, 658, 441, 607, 768, 324, 515,  15, 791, 335, 758, 257,\n       496, 987, 895, 159, 474, 851, 663, 907, 689, 674, 379, 544, 928,\n       956, 690, 426, 612, 267, 834, 576, 416, 167, 841,  42, 555, 284,\n       396,  11, 606, 301, 897, 252, 498, 753,  34, 726, 848,  89, 775,\n       604, 921, 969, 601, 417, 114, 616, 902, 195, 825, 500, 625, 492,\n        74, 412, 375, 419, 728, 276, 760, 675, 393, 868, 456, 919, 191,\n       738, 688,  98, 547, 977, 998,  95, 663, 662, 189, 735,  36, 779,\n       368, 694, 524, 278, 216, 866, 872, 797, 272, 880,  61, 595, 879,\n       728, 341, 396, 698,  18, 176, 611, 395, 444, 232, 914,  75, 264,\n       454, 795, 717, 734, 383, 563, 850, 505, 366, 143, 884,  68,  98,\n       395,  24, 947, 890, 468, 483, 564, 150, 143, 568,  38, 108, 692,\n        41, 185, 934, 397, 222, 633, 132, 162, 214, 732, 234, 842, 657,\n       750, 587,   8,  73, 953, 491, 912, 252, 229, 518, 173, 652, 167,\n       169, 392, 945, 794, 633, 193, 516,  28, 164, 421, 338, 647, 495,\n       364, 832, 341, 499, 656, 510, 326, 216, 300, 131, 803,  69, 251,\n       414, 786, 444, 875, 181, 166,  90, 713, 857, 530,  38, 125, 450,\n       172, 652, 753, 219, 637,  57, 659, 475, 455, 828, 894, 360, 934,\n         0, 386, 972, 347, 189, 504, 190, 507, 368, 408, 823, 928, 933,\n       116, 133,  57, 555, 684, 671, 172, 828, 814, 148,  79, 885, 212,\n       202, 763, 228, 675, 226, 658, 531, 440, 401,  46, 232, 304, 525,\n       142, 414, 512, 372, 565, 885, 258, 655, 470, 952, 970,  11, 329,\n       735, 783, 967, 357, 971, 407, 667, 372,   7, 121, 347, 675,  89,\n       647, 697, 315, 177, 539, 731, 868,  40, 739, 703, 922, 501, 958,\n       144, 200, 928, 723, 460, 731, 751, 924, 908, 557, 546, 252, 389,\n       593, 882, 255, 708, 814, 920, 449,   9, 823, 797, 241, 250])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mg:\\github\\topicflow\\test.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/github/topicflow/test.ipynb#Y220sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m v \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m25\u001b[39m,   \u001b[39m1000\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/github/topicflow/test.ipynb#Y220sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, j, l \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(d, k, v):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/github/topicflow/test.ipynb#Y220sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     test_element \u001b[39m=\u001b[39m Theta[: W[d, i]]\n",
      "File \u001b[1;32mc:\\Users\\janni\\anaconda3\\envs\\tfa-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\janni\\anaconda3\\envs\\tfa-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:907\u001b[0m, in \u001b[0;36m_check_index\u001b[1;34m(idx)\u001b[0m\n\u001b[0;32m    902\u001b[0m dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(idx, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    903\u001b[0m \u001b[39mif\u001b[39;00m (dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m dtypes\u001b[39m.\u001b[39mas_dtype(dtype) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _SUPPORTED_SLICE_DTYPES \u001b[39mor\u001b[39;00m\n\u001b[0;32m    904\u001b[0m     idx\u001b[39m.\u001b[39mshape \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(idx\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m    905\u001b[0m   \u001b[39m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[39;00m\n\u001b[0;32m    906\u001b[0m   \u001b[39m# will break `_slice_helper` contract.\u001b[39;00m\n\u001b[1;32m--> 907\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(_SLICE_TYPE_ERROR \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, got \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(idx))\n",
      "\u001b[1;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([102, 435, 860, 270, 106,  71, 700,  20, 614, 121, 466, 214, 330,\n       458,  87, 372,  99, 871, 663, 130, 661, 308, 769, 343, 491, 413,\n       805, 385, 191, 955, 276, 160, 459, 313,  21, 252, 747, 856, 560,\n       474,  58, 510, 681, 475, 699, 975, 782, 189, 957, 686, 957, 562,\n       875, 566, 243, 831, 504, 130, 484, 818, 646,  20, 840, 166, 273,\n       387, 600, 315,  13, 241, 776, 345, 564, 897, 339,  91, 366, 955,\n       454, 427, 508, 775, 942,  34, 205,  80, 931, 561, 871, 387,   1,\n       389, 565, 105, 771, 821, 476, 702, 401, 729, 555, 161, 201, 957,\n       995, 269, 862, 815, 270, 455, 461, 726, 251, 701, 295, 724, 719,\n       748, 337, 878,  52, 791, 921, 216, 763, 187, 379, 492,  40, 156,\n        14, 812,  64, 856, 838, 520, 343, 128, 647, 471,  62, 138, 498,\n       592, 391, 674, 418, 288, 378, 772, 489, 230,  40,  27, 134, 200,\n       839, 779, 929,  32,  47, 502, 406, 573, 727, 804,  98, 683, 871,\n       725, 986, 546, 960, 738, 612, 942, 461, 642, 768,   4, 217, 502,\n       766, 397, 870, 794, 392, 206,  14, 857, 553, 891, 460, 690, 574,\n       863, 742, 240, 563,  95, 899, 733, 484, 406, 230, 748, 654, 170,\n       540,  35, 524, 159, 838, 698, 242,  85, 795, 577, 681, 556, 573,\n       952, 645, 795,  27, 619, 555, 339, 797, 957, 330, 639, 505, 347,\n       472, 230, 189, 224, 384, 376, 282, 957, 632, 627, 972, 744, 258,\n       358, 709, 455, 410, 648, 317, 676, 224, 818, 233, 683, 663, 974,\n       826, 373, 671, 607, 471, 232, 691, 112, 829, 496, 441, 563, 267,\n       509, 806, 385, 386, 112, 612, 624, 951,  80, 698, 112,   1, 641,\n       219, 565, 854, 996, 735, 224, 384, 402, 637, 129,  52, 683, 729,\n       671, 709, 415, 246, 835, 438, 202, 183, 122, 400, 766, 293, 279,\n       836, 883, 609, 197, 981, 906, 510, 751, 143, 608, 200, 123, 186,\n       325, 463, 348, 770, 659, 763, 954, 931, 402, 345, 962, 510, 146,\n       147, 863, 710, 819, 488, 928, 935, 639, 550, 337, 871, 640, 778,\n       987, 952, 472, 945, 150, 414, 989, 297, 610, 262, 763, 143, 345,\n       623, 571, 880,   1, 896, 303, 253, 651, 452,  36, 159,   8, 232,\n        98, 658, 815, 207, 130, 403, 151,  53, 119, 672, 919, 627, 586,\n       624, 967, 419, 421, 103, 851, 253, 226, 111, 509, 472,  98, 152,\n       860, 913, 895, 877, 337, 705, 821, 162, 719, 956, 680, 995, 160,\n       579, 800, 397, 276, 815, 915, 503, 895, 391, 134, 194, 400, 639,\n        32, 687, 459, 954, 882, 469, 374,  21, 749, 669,  37, 229, 364,\n       562, 437, 775, 282,  26, 225, 276, 797, 608, 283, 878, 959, 480,\n       452, 828, 815, 658, 515, 546, 191,  48, 511,  16, 171, 219, 157,\n       476,  45, 372, 517,  98, 891, 744,  36, 279, 348, 496, 301, 180,\n       606,  98, 699, 992, 115, 190, 252, 980, 927, 982, 160, 255, 322,\n       127,  17, 792, 734, 565, 569, 322, 871, 685, 791, 625, 287, 942,\n       853, 662, 961, 638, 154, 489, 385, 985, 784, 103, 928, 392, 810,\n       245, 175,  38, 476, 681, 758, 537, 866, 817, 920, 407, 524, 827,\n       505, 902, 824,  35, 684,  19, 320, 775, 511, 399, 653, 971, 882,\n       470, 142,  91, 353, 833, 799, 726, 958, 853,  50, 664, 697, 574,\n       189, 124, 149, 313, 569, 341, 304, 691, 681, 837, 782,  53, 443,\n       612, 992, 263,  52, 571, 619,   4, 102, 195, 773, 876, 991, 883,\n       349,  46, 866, 822, 935, 819, 655, 268, 369, 635, 105, 669, 658,\n       656, 119, 830, 786, 603,  57, 950, 345, 740, 473, 116, 829, 790,\n       126, 392, 907, 640,  57, 633, 512, 750, 801,  95, 637, 117, 559,\n       600, 487, 236, 884, 896, 271, 188, 998, 703, 446, 580, 789, 860,\n       246, 962,  75, 153, 655, 434, 996,  85, 696, 284, 973, 219,  68,\n        46,  93, 749, 957, 452, 203, 911, 217, 473, 431, 340, 550, 611,\n       288, 253, 733, 356,  22, 761, 521, 757, 836,  99, 801, 179, 222,\n       905, 761, 658, 441, 607, 768, 324, 515,  15, 791, 335, 758, 257,\n       496, 987, 895, 159, 474, 851, 663, 907, 689, 674, 379, 544, 928,\n       956, 690, 426, 612, 267, 834, 576, 416, 167, 841,  42, 555, 284,\n       396,  11, 606, 301, 897, 252, 498, 753,  34, 726, 848,  89, 775,\n       604, 921, 969, 601, 417, 114, 616, 902, 195, 825, 500, 625, 492,\n        74, 412, 375, 419, 728, 276, 760, 675, 393, 868, 456, 919, 191,\n       738, 688,  98, 547, 977, 998,  95, 663, 662, 189, 735,  36, 779,\n       368, 694, 524, 278, 216, 866, 872, 797, 272, 880,  61, 595, 879,\n       728, 341, 396, 698,  18, 176, 611, 395, 444, 232, 914,  75, 264,\n       454, 795, 717, 734, 383, 563, 850, 505, 366, 143, 884,  68,  98,\n       395,  24, 947, 890, 468, 483, 564, 150, 143, 568,  38, 108, 692,\n        41, 185, 934, 397, 222, 633, 132, 162, 214, 732, 234, 842, 657,\n       750, 587,   8,  73, 953, 491, 912, 252, 229, 518, 173, 652, 167,\n       169, 392, 945, 794, 633, 193, 516,  28, 164, 421, 338, 647, 495,\n       364, 832, 341, 499, 656, 510, 326, 216, 300, 131, 803,  69, 251,\n       414, 786, 444, 875, 181, 166,  90, 713, 857, 530,  38, 125, 450,\n       172, 652, 753, 219, 637,  57, 659, 475, 455, 828, 894, 360, 934,\n         0, 386, 972, 347, 189, 504, 190, 507, 368, 408, 823, 928, 933,\n       116, 133,  57, 555, 684, 671, 172, 828, 814, 148,  79, 885, 212,\n       202, 763, 228, 675, 226, 658, 531, 440, 401,  46, 232, 304, 525,\n       142, 414, 512, 372, 565, 885, 258, 655, 470, 952, 970,  11, 329,\n       735, 783, 967, 357, 971, 407, 667, 372,   7, 121, 347, 675,  89,\n       647, 697, 315, 177, 539, 731, 868,  40, 739, 703, 922, 501, 958,\n       144, 200, 928, 723, 460, 731, 751, 924, 908, 557, 546, 252, 389,\n       593, 882, 255, 708, 814, 920, 449,   9, 823, 797, 241, 250])"
     ]
    }
   ],
   "source": [
    "test_tensor = tf.cast(Theta[:, W], dtype=tf.float32)\n",
    "Theta = tf.cast(Theta, dtype=tf.float32)\n",
    "\n",
    "d = np.random.randint(0, 1000, 1000)\n",
    "k = np.random.randint(0, 10,   1000)\n",
    "v = np.random.randint(0, 25,   1000)\n",
    "\n",
    "for i, j, l in zip(d, k, v):\n",
    "    test_element = Theta[: W[d, i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numerator\n",
    "#  Pi_{dk} Theta_{kv} for d=0, k=:, v=0\n",
    "test_line = tf.math.multiply(Pi[0, :], Theta[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000, 10, 25])"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta_block = tf.cast(tf.stack(D*[Theta]), dtype=tf.float32)\n",
    "Theta_block.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000, 10, 25])"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pi_block = tf.cast(tf.stack(V*[Pi], axis=-1), dtype=tf.float32)\n",
    "Pi_block.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = tf.math.multiply(Pi_block, Theta_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.random.randint(0, 1000, 1000)\n",
    "k = np.random.randint(0, 10,   1000)\n",
    "v = np.random.randint(0, 25,   1000)\n",
    "\n",
    "for i, j, l in zip(d, k, v):\n",
    "    test_element = Pi[i, j] * Theta[j, l]\n",
    "    assert numerator[i, j, l] == test_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000, 10, 25])"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerator.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Shapes of all inputs must match: values[0].shape = [] != values[2].shape = [1000,100] [Op:Pack]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mg:\\github\\topicflow\\test.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/github/topicflow/test.ipynb#Y216sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m numerator[:, :, W]\n",
      "File \u001b[1;32mc:\\Users\\janni\\anaconda3\\envs\\tfa-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\janni\\anaconda3\\envs\\tfa-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7164\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7163\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7164\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Shapes of all inputs must match: values[0].shape = [] != values[2].shape = [1000,100] [Op:Pack]"
     ]
    }
   ],
   "source": [
    "numerator[:, :, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1000, 25), dtype=float32, numpy=\n",
       "array([[0.01096468, 0.025155  , 0.05349239, ..., 0.08890645, 0.07944958,\n",
       "        0.05879878],\n",
       "       [0.01642036, 0.02460816, 0.08911462, ..., 0.09455124, 0.039477  ,\n",
       "        0.03320314],\n",
       "       [0.00542791, 0.00209033, 0.00692034, ..., 0.05183261, 0.07215814,\n",
       "        0.08488888],\n",
       "       ...,\n",
       "       [0.06013564, 0.06642435, 0.07992691, ..., 0.04427514, 0.04641789,\n",
       "        0.01641715],\n",
       "       [0.01483393, 0.00995105, 0.00878754, ..., 0.07429013, 0.10092726,\n",
       "        0.07438865],\n",
       "       [0.03969378, 0.03561527, 0.09280664, ..., 0.06825793, 0.03933832,\n",
       "        0.02731759]], dtype=float32)>"
      ]
     },
     "execution_count": 756,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(numerator, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1000, 10), dtype=int32, numpy=\n",
       "array([[[10, 23, 14, ..., 18, 23, 15],\n",
       "        [ 5,  1,  5, ..., 23,  7,  7],\n",
       "        [ 3,  5, 12, ...,  7, 13, 13],\n",
       "        ...,\n",
       "        [18, 10,  4, ..., 10,  4,  0],\n",
       "        [ 1, 17,  6, ..., 10,  5,  4],\n",
       "        [12, 10,  2, ...,  1,  2, 18]],\n",
       "\n",
       "       [[11, 18,  1, ...,  8, 10,  2],\n",
       "        [ 1,  7,  4, ..., 24,  6, 23],\n",
       "        [ 6, 16,  4, ..., 13, 17,  8],\n",
       "        ...,\n",
       "        [18, 16, 16, ...,  5, 18, 12],\n",
       "        [ 8, 12, 11, ...,  9,  6, 21],\n",
       "        [22,  9, 15, ..., 10,  5, 11]],\n",
       "\n",
       "       [[10,  8,  1, ..., 13,  5, 22],\n",
       "        [19, 19,  1, ...,  5, 15, 11],\n",
       "        [ 5, 14,  5, ..., 24,  1, 12],\n",
       "        ...,\n",
       "        [18, 13, 11, ..., 24, 13,  2],\n",
       "        [ 7, 14, 21, ...,  7, 19,  2],\n",
       "        [21, 23, 10, ...,  4,  8,  7]],\n",
       "\n",
       "       [[20, 18,  7, ..., 17,  2, 10],\n",
       "        [ 1, 17, 18, ...,  9,  5,  1],\n",
       "        [ 6,  6, 20, ..., 16,  1, 18],\n",
       "        ...,\n",
       "        [ 0, 18,  7, ..., 24,  3,  5],\n",
       "        [ 2, 17, 13, ..., 22, 19, 20],\n",
       "        [ 1,  3, 10, ...,  0,  9, 15]]])>"
      ]
     },
     "execution_count": 757,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfd.Categorical(numerator).sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0])>"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfd.Categorical([[0.01, 0.01, 0.98], [0.98, 0.01, 0.01]]).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling $\\Theta$ and $\\Pi$\n",
    "\n",
    "Sampling $\\Theta$ and $\\Pi$:\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        p(\\Theta\\vert C, W) &= \\prod_{k=1}^K \\mathcal D(\\theta_k; \\ \\beta_{k:} + n_{\\cdot k:}) \\\\\n",
    "        p(\\Pi\\vert C, W)    &= \\prod_{d=1}^D \\mathcal D(\\pi_d; \\ \\alpha_{d:} + n_{d:\\cdot})\n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sample_Theta(N_DKV, beta):\n",
    "    dist_Theta = tfd.Dirichlet(beta + tf.reduce_sum(N_DKV, axis=0))\n",
    "    Theta      = dist_Theta.sample()\n",
    "    return Theta\n",
    "\n",
    "@tf.function\n",
    "def sample_Pi(N_DKV, alpha):\n",
    "    dist_Pi = tfd.Dirichlet(alpha + tf.reduce_sum(N_DKV, axis=-1))\n",
    "    Pi      = dist_Pi.sample()\n",
    "    return Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Initialization of Prior Alpha and Beta.\n",
    "#  They do not acutally matter much but could be optimized by Type II MAP or MLE.\n",
    "beta  = tf.constant(np.random.normal(size=(K, V)), dtype=tf.float32)\n",
    "alpha = tf.constant(np.random.normal(size=(D, K)), dtype=tf.float32)\n",
    "\n",
    "## For Test purposes: Calculate one N_tensor:\n",
    "N_DKV = tf_N_tensor(tf.Variable(W), tf.Variable(C_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Theta = sample_Theta(N_DKV, beta)\n",
    "test_Pi    = sample_Pi(N_DKV, alpha)\n",
    "\n",
    "assert test_Theta.shape == Theta.shape\n",
    "assert test_Pi.shape == Pi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_iter = 200\n",
    "Theta_store = []\n",
    "W_ = W\n",
    "\n",
    "C0         = np.random.randint(0, 10,  size=(D, N))\n",
    "C0_one_hot = tf.one_hot(C0, depth=K, axis=-1)\n",
    "\n",
    "alpha0 = np.random.randint(0, 10,  size=(D, N))\n",
    "beta0  = np.random.normal(size=(D, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for iter in  tqdm(range(N_iter)):\n",
    "#     Ndkv = \n",
    "#     Theta_it = tfd.Dirichlet()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tfa-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b28dcc1f87ebe436630d7ecff3a3c61833e6c22febb81b1e7cd0d34da649d2b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

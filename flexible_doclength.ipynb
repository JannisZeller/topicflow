{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Model\n",
    "\n",
    "Trying toy model inspired by [Griffiths & Steyvers, 2004](https://doi.org/10.1073/pnas.0307752101)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "tfd = tfp.distributions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from timeit import timeit\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One hot for numpy\n",
    "def np_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sampling Behaviour of `tfp.Distributions`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 0],\n",
       "       [2, 0],\n",
       "       [2, 0],\n",
       "       [2, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tfd.Categorical sampling behaviour:\n",
    "probs = tf.constant([[0., 0., 1.], \n",
    "                     [1., 0., 0.]])\n",
    "print(probs.shape)\n",
    "tfd.Categorical(probs=probs).sample(4).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02140409, 0.01427036, 0.9643255 ],\n",
       "       [0.38862276, 0.16176145, 0.44961584]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tfd.Dirichlet sampling behaviour:\n",
    "conc = tf.constant([[0.1, 0.1, 0.1], \n",
    "                    [  2.,  2.,  2.]])\n",
    "tfd.Dirichlet(conc).sample().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ Samples come in by row. For Categorical the sample size is flexible. For Dirichlet the sample cise is bounded to the concentrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting up Topics and Data**\n",
    "\n",
    "### Understand The indices in this Notebook\n",
    "\n",
    "- $K$ is the number of topics \n",
    "- $D$ is the number of documents\n",
    "- $I_d$ is the number of words in document $d$\n",
    "- $N_{\\mathrm{max}}$ is the maximum number of words per doument, i. e. $N_{\\mathrm{max}} = \\max_d \\{I_d\\}$\n",
    "- $T$ is the total number of words, i. e. $T = \\sum_d I_d$\n",
    "- $V$ is the vocabulary size which should be a square of an integer for the visualization purposes of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Specify global parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constructing iterator for single documents N's\n",
    "class sliceConverter(Sequence):\n",
    "    def __init__(self, N_D: tf.Tensor):\n",
    "        assert N_D.ndim == 1\n",
    "        N_idx = tf.cumsum(N_D)\n",
    "        N_idx = tf.repeat(N_idx, 2)\n",
    "        self.N_idx   = tf.concat([[0], N_idx], axis=0).numpy()\n",
    "        self.N_max   = tf.reduce_mean(N_D).numpy()\n",
    "        self.N_total = N_idx[-1].numpy()\n",
    "        self.single_lengths = N_D\n",
    "\n",
    "    def __len__(self):\n",
    "        return int((self.N_idx.shape[0] - 1) / 2)\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        # if type(k) == int:\n",
    "        #     if k >= 0:\n",
    "        #         return tf.range(self.N_idx[2*k], self.N_idx[2*k + 1])\n",
    "        #     if k < 0:\n",
    "        #         i = len(self) + k\n",
    "        #         return tf.range(self.N_idx[2*i], self.N_idx[2*i + 1])\n",
    "        # elif type(k) == slice:\n",
    "        #     start, stop, step = k.indices(len(self))\n",
    "        #     return [self[i] for i in range(start, stop, step)]\n",
    "        if type(k) == int:\n",
    "            if k >= 0:\n",
    "                return slice(self.N_idx[2*k], self.N_idx[2*k + 1], 1)\n",
    "            if k < 0:\n",
    "                i = len(self) + k\n",
    "                return slice(self.N_idx[2*i], self.N_idx[2*i + 1], 1)\n",
    "        elif type(k) == slice:\n",
    "            start, stop, step = k.indices(len(self))\n",
    "            return [self[i] for i in range(start, stop, step)]\n",
    "        else:\n",
    "            raise TypeError(\"Index must be integer or slice.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.idx = 0\n",
    "        self.maxiter = len(self)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.idx < self.maxiter:\n",
    "            ret = self.__getitem__(self.idx)\n",
    "            self.idx += 1\n",
    "            return ret\n",
    "        else:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "K       = 10    # Number of Topics\n",
    "sqrtV   = 5     # Square Root of the Number of \"Vocabulary\" (must be sqrt such that pictorial interpretation is possible)\n",
    "D       = 1000  # Number of documents\n",
    "N_rate  = None   # Number of words per document (either as rate for Poisson Distribution\n",
    "N_fixed = 100  #   or as fixed value)\n",
    "\n",
    "V = int(sqrtV**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Generating Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_LDA(D: int, K: int, sqrtV: int, N_fixed: int=None, N_rate: int=None) -> tuple():\n",
    "    \"\"\"\n",
    "    Generates LDA-Data from the LDA generative Process. \n",
    "    Thetas get constructed as rows and columns of a quadratic sqrtV-grid.\n",
    "    Either N_fixed ot N_rate has to be passed\n",
    "    \"\"\"\n",
    "    assert (N_fixed is None) != (N_rate is None), \"Exactly one of N_fixed and N_rate must be passed.\"\n",
    "\n",
    "\n",
    "    ## Number of words per document\n",
    "    if N_rate is not None:\n",
    "        N_D_dist = tfd.Poisson(rate=100)\n",
    "        N_D      = tf.cast(N_D_dist.sample(D), dtype=tf.int32)\n",
    "    if N_fixed is not None:\n",
    "        N_D = np.array(D*[N_fixed])\n",
    "    N_idx    = sliceConverter(N_D)\n",
    "    \n",
    "\n",
    "    ## Word grid\n",
    "    V = int(sqrtV**2)\n",
    "    V_grid = np.reshape(np.arange(0, V), newshape=(sqrtV, sqrtV))\n",
    "\n",
    "\n",
    "    ## Topic-Word Distribution\n",
    "    #  Words belonging to a topic are rows and columns\n",
    "    Theta_idx = [row for row in V_grid] + [col for col in V_grid.T]\n",
    "    Theta = np.zeros((K, V))\n",
    "    for k, idx in enumerate(Theta_idx):\n",
    "        Theta[k, idx] = 1. / sqrtV\n",
    "    Theta = tf.constant(Theta, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    ## Document-Topic Distribution\n",
    "    #  Initializing Alpha manually to 1.\n",
    "    Alpha = 1\n",
    "    dist_Pi = tfd.Dirichlet(K*[Alpha])\n",
    "    Pi      = dist_Pi.sample(D)\n",
    "\n",
    "\n",
    "    ## Topic Assignments of word c_{dik} of word w_{di}\n",
    "    dist_C  = tfd.Categorical(probs=Pi)\n",
    "    C_NmaxD = dist_C.sample(tf.reduce_max(N_D))\n",
    "\n",
    "    C_T = []\n",
    "    for d in range(D):\n",
    "        C_T_col = C_NmaxD[:N_D[d], d]\n",
    "        C_T.append(C_T_col)\n",
    "    C_T = tf.concat(C_T, axis=0)\n",
    "    C_TK = tf.one_hot(C_T, depth=K, axis=-1)\n",
    "\n",
    "\n",
    "    ## Draw words w_{di}\n",
    "    dist_W_T = tfd.Categorical(probs=tf.gather(Theta, C_T))\n",
    "    W_T      = dist_W_T.sample()\n",
    "\n",
    "\n",
    "    return Theta, Pi, C_T, C_TK, W_T, N_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta, Pi, C_T, C_TK, W_T, N_idx = generate_LDA(D, K, sqrtV, N_fixed=N_fixed, N_rate=N_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics for the Words in Document d=11:\n",
      "tf.Tensor(\n",
      "[1 6 1 5 8 8 8 6 4 1 6 4 5 3 6 5 9 6 4 5 2 3 5 5 1 5 9 4 5 4 8 6 1 4 9 8 3\n",
      " 5 5 8 3 5 8 5 6 5 3 9 1 9 4 6 6 8 9 5 4 6 4 3 5 9 6 7 9 8 5 2 4 4 7 9 9 4\n",
      " 5 7 6 5 5 8 3 9 4 5 8 6 8 3 5 4 5 7 9 3 5 6 5 6 3 4], shape=(100,), dtype=int32)\n",
      "\n",
      "Words in Document d=11:\n",
      "tf.Tensor(\n",
      "[ 5 11  5 15  3 18 23  1 23  7  6 23 10 19  1  5  4 21 20  5 12 19  0 15\n",
      "  7  5  9 23  5 24 18 11  9 20 19 18 18 15 15 23 17  5 13 20 11 15 18  4\n",
      "  5  9 24 21  1  3 24 15 20  1 24 17 10  4 16  7 19 23 15 12 24 23  7 19\n",
      "  4 23 20  7 21  0 10 18 19 14 21 20  8 21  3 15 10 20 15 17  9 16 10 21\n",
      " 15 21 16 22], shape=(100,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "## To extract topic-word assignments and document-words for a \n",
    "#  specific document, use the slice converter:\n",
    "doc_slice = N_idx[11]\n",
    "\n",
    "print(\"Topics for the Words in Document d=11:\")\n",
    "print(C_T[doc_slice])\n",
    "print(\"\\nWords in Document d=11:\")\n",
    "print(W_T[doc_slice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Checking Shapes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For development purposes:\n",
    "def print_all_shapes():\n",
    "    print(\"Π-Shape (D docs x K topics)\")\n",
    "    print(Pi.shape)\n",
    "    print(\"\\nΘ-Shape (K topic x V vocab)\")\n",
    "    print(Theta.shape)\n",
    "    print(\"\\nC_T-Shape (Topic for each word T=sum_d I_d)\")\n",
    "    print(C_T.shape)\n",
    "    print(\"\\nC_TK-Shape (one-hot for each topic k in {1, K})\")\n",
    "    print(C_TK.shape)\n",
    "    print(\"\\nW-Shape (sum_d I_d words)\")\n",
    "    print(W_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Π-Shape (D docs x K topics)\n",
      "(1000, 10)\n",
      "\n",
      "Θ-Shape (K topic x V vocab)\n",
      "(10, 25)\n",
      "\n",
      "C_T-Shape (Topic for each word T=sum_d I_d)\n",
      "(100000,)\n",
      "\n",
      "C_TK-Shape (one-hot for each topic k in {1, K})\n",
      "(100000, 10)\n",
      "\n",
      "W-Shape (sum_d I_d words)\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "print_all_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_to_img(arr, sqrtV=sqrtV):\n",
    "    vals = dict(zip(*np.unique(arr, return_counts=True)))\n",
    "    img = []\n",
    "\n",
    "    for i in range(int(sqrtV**2)):\n",
    "        if i in vals:\n",
    "            img.append(vals[i])\n",
    "        else:\n",
    "            img.append(0)\n",
    "\n",
    "    img = np.array(img).reshape(sqrtV, sqrtV)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([0.38780424, 0.18907806, 0.02911452, 0.06413591, 0.19551848,\n",
       "       0.0772825 , 0.03335454, 0.00486895, 0.01072818, 0.00811451],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pi[10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2be44fb4640>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(format_to_img(W_T[N_idx[10]]), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2be4508dc60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.reshape(Theta[0,:], (sqrtV, sqrtV)), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gibbs Sampler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **N-Tensor**\n",
    "One efficiency crtitical step is to vectorize \n",
    "$$\n",
    "    n_{dkv} =  \\{i \\, \\vert \\, w_{di} == v \\ \\& \\ c_{idk} ==1\\}\n",
    "$$\n",
    "as much as possible. Due to the fact, that the document lengths are variable there is the choice between looping over the number of documents or padding the documents to a unique length. The padding does not need a word-token, it can be done by padding the $C$ matrix with $0$, therefore it will not affect $N$. Then the counting can be realized by stacking $W$ $K$ times along the last axis to match the shape of $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Multiloop over all Dimensions\n",
    "\n",
    "Mainly for test purposes. Horribly slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiloop_N_tensor(W_T, C_TK, N_idx, D=D, V=V):\n",
    "    K    = C_TK.shape[1]\n",
    "    W_T  = W_T.numpy()\n",
    "    C_TK = C_TK.numpy()\n",
    "    N_DKV = np.zeros(shape=(D, K, V))\n",
    "    \n",
    "    for d in tqdm(range(D)):\n",
    "        di = N_idx[d]\n",
    "        for k in range(K):\n",
    "            for v in range(V):\n",
    "                for w_di, c_dik in zip(W_T[di], C_TK[di, k]):\n",
    "                    if w_di == v and c_dik == 1:\n",
    "                        N_DKV[d, k, v] += 1\n",
    "\n",
    "    return N_DKV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test and Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_test1 = multiloop_N_tensor(W_T, C_TK, N_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit multiloop_N_tensor(W_T, C_TK, N_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Single-Loop over $D$ with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleloop_N_tensor(W_T, C_TK, N_idx, D=D, V=V, verbose=False):\n",
    "    K = C_TK.shape[1]\n",
    "    W_T   = W_T.numpy()\n",
    "    C_TK  = C_TK.numpy()\n",
    "    N_DKV = np.zeros(shape=(D, K, V))\n",
    "\n",
    "    ## Preparing W-stacking by shifting all entries one \"up\" s. t. v is counted \n",
    "    #  from 1 to V+1 instead from 0 to V. This enables to collapse the \"&\" in the\n",
    "    #  set to be collapsed to a matrix product\n",
    "    Wp1 = W_T + 1\n",
    "    \n",
    "    if verbose:\n",
    "        iter = tqdm(range(D))  \n",
    "    if not verbose:\n",
    "        iter = range(D)\n",
    "\n",
    "    for d in iter:\n",
    "        di = N_idx[d]\n",
    "        W_ = Wp1[di]\n",
    "        C_TK_ = C_TK[di]\n",
    "\n",
    "        Nd = len(W_)\n",
    "\n",
    "        ## Stacking W\n",
    "        W_stacked = np.stack(K*[W_], axis=-1)\n",
    "\n",
    "        ## Elementwise product combines logical & in conditio\n",
    "        #  Choosing int32 as product dtype for efficiency.\n",
    "        C_Dot_W = np.multiply(W_stacked, C_TK_)\n",
    "\n",
    "        ## The v-dimension of N is a one-hot encoding for the vocabulary:\n",
    "        N_dNdKVp1 = np_one_hot(C_Dot_W.astype(np.int32), V+1)\n",
    "\n",
    "        ## Reverting the v-shift by dropping the 0 one-hot dimension\n",
    "        N_dNdKV = N_dNdKVp1[:, :, 1:]\n",
    "\n",
    "        ## Summing along v-dimension\n",
    "        assert N_dNdKV.shape[0] == Nd\n",
    "        N_dKV = np.sum(N_dNdKV, axis=0)\n",
    "\n",
    "        ## Append to full tensor\n",
    "        N_DKV[d, :, :] = N_dKV\n",
    "\n",
    "    return N_DKV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test and Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test2 = singleloop_N_tensor(W_T, C_TK, N_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit singleloop_N_tensor(W, C_TK, N_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy implementation which avoides all loops but the loop over documents is about a factor 63.2 more efficient as multiloop version.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numpy implementation which avoides all loops but the loop over documents is about a factor {round(8150./129., 1)} more efficient as multiloop version.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Vectorized with Padding using `tf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding Functions\n",
    "def fungen_pad_CW(N_idx):\n",
    "    \n",
    "    @tf.function\n",
    "    def padd_CW(W_T, C_TK):\n",
    "        K = C_TK.shape[1]\n",
    "\n",
    "        ## Reshaping C_TK to C_DNmaxK and\n",
    "        #            W_T  to W_DNmax\n",
    "        C_DNmaxK_list = []\n",
    "        W_DNmax_list  = []\n",
    "        N_max = tf.reduce_max(N_idx.single_lengths)\n",
    "\n",
    "        for di in N_idx:\n",
    "            CdiK = C_TK[di, :]\n",
    "            Wdi  = W_T[di]\n",
    "            Nd   = CdiK.shape[0]\n",
    "\n",
    "            N_pad = N_max - Nd\n",
    "\n",
    "            ## C\n",
    "            pad = tf.zeros(shape=(N_pad, K), dtype=CdiK.dtype)\n",
    "            CdiK_pad = tf.concat([CdiK, pad], axis=0)\n",
    "            C_DNmaxK_list.append(CdiK_pad)\n",
    "\n",
    "            ## W\n",
    "            pad = tf.zeros(shape=(N_pad), dtype=Wdi.dtype)\n",
    "            Wdi_pad = tf.concat([Wdi, pad], axis=0)\n",
    "            W_DNmax_list.append(Wdi_pad)\n",
    "\n",
    "        C_DNmaxK = tf.stack(C_DNmaxK_list, axis=0)\n",
    "        W_DNmax  = tf.stack(W_DNmax_list, axis=0)\n",
    "\n",
    "        return W_DNmax, C_DNmaxK\n",
    "\n",
    "    return padd_CW\n",
    "\n",
    "## Using a tf.function decorator causes the creation to take pretty long because a graph\n",
    "#  is built which has to take the document lengths into account. This might be only \n",
    "#  useful if the function itself takes long to run without a tf.function... Maybe\n",
    "#  Better use just the plain-non tf.function-converted:\n",
    "\n",
    "def padd_CW(W_T, C_TK, N_idx):\n",
    "    K = C_TK.shape[1]\n",
    "\n",
    "    ## Reshaping C_TK to C_DNmaxK and\n",
    "    #            W_T  to W_DNmax\n",
    "    C_DNmaxK_list = []\n",
    "    W_DNmax_list  = []\n",
    "    N_max = tf.reduce_max(N_idx.single_lengths)\n",
    "\n",
    "    for di in N_idx:\n",
    "        CdiK = C_TK[di, :]\n",
    "        Wdi  = W_T[di]\n",
    "        Nd   = CdiK.shape[0]\n",
    "\n",
    "        N_pad = N_max - Nd\n",
    "\n",
    "        ## C\n",
    "        pad = tf.zeros(shape=(N_pad, K), dtype=CdiK.dtype)\n",
    "        CdiK_pad = tf.concat([CdiK, pad], axis=0)\n",
    "        C_DNmaxK_list.append(CdiK_pad)\n",
    "\n",
    "        ## W\n",
    "        pad = tf.zeros(shape=(N_pad), dtype=Wdi.dtype)\n",
    "        Wdi_pad = tf.concat([Wdi, pad], axis=0)\n",
    "        W_DNmax_list.append(Wdi_pad)\n",
    "\n",
    "    C_DNmaxK = tf.stack(C_DNmaxK_list, axis=0)\n",
    "    W_DNmax  = tf.stack(W_DNmax_list, axis=0)\n",
    "\n",
    "    return W_DNmax, C_DNmaxK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Actual n-Tensor\n",
    "@tf.function\n",
    "def tf_N_tensor(W_DNmax, C_DNmaxK, V=V):\n",
    "\n",
    "    ## Extracting shapes\n",
    "    D = C_DNmaxK.shape[0]\n",
    "    N = C_DNmaxK.shape[1]\n",
    "    K = C_DNmaxK.shape[2]\n",
    "    \n",
    "    ## Preparing W-stacking by shifting all entries one \"up\" s. t. v is counted \n",
    "    #  from 1 to V+1 instead from 0 to V. This enables to collapse the \"&\" in the\n",
    "    #  set to be collapsed to a matrix product\n",
    "    Wp1 = W_DNmax + 1\n",
    "    W_stacked = tf.stack(K*[Wp1], axis=-1)    \n",
    "\n",
    "    ## Elementwise product combines logical & in condition.\n",
    "    #  Choosing int32 as product dtype for efficiency.\n",
    "    C_DNmaxK_int = tf.cast(C_DNmaxK, dtype=tf.int32)\n",
    "    C_Dot_W = tf.math.multiply(W_stacked, C_DNmaxK_int)\n",
    "\n",
    "    ## The v-dimension of N is a one-hot encoding for the vocabulary:\n",
    "    N_DNKVp1 = tf.one_hot(C_Dot_W, V+1, dtype=tf.int32)\n",
    "\n",
    "    ## Reverting the v-shift by dropping the 0 one-hot dimension\n",
    "    N_DNKV = N_DNKVp1[:, :, :, 1:]\n",
    "\n",
    "    ## Summing along v-dimension\n",
    "    assert N_DNKV.shape[1] == N\n",
    "    N_DKV = tf.reduce_sum(N_DNKV, axis=1)\n",
    "\n",
    "    ## Turn to float for gibbs sampler\n",
    "    N_DKV = tf.cast(N_DKV, dtype=tf.float32)\n",
    "\n",
    "    return N_DKV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test and Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit tf_padd_CW(W, C_TK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit padd_CW(W, C_TK, N_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a tf.function the padding is about a factor 15.4 more efficient, when the graph is already constructed. Yet constructing the graph takes a while.\n"
     ]
    }
   ],
   "source": [
    "print(f\"With a tf.function the padding is about a factor {round(970./63., 1)} more efficient, when the graph is already constructed. Yet constructing the graph takes a while.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_DNmax, C_DNmaxK = padd_CW(W_T, C_TK, N_idx)\n",
    "# tf_padd_CW = fungen_pad_CW(N_idx)\n",
    "# W_DNmax, C_DNmaxK = tf_padd_CW(W_T, C_TK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Tensor itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit tf_N_tensor(W_DNmax, C_DNmaxK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The padded tf Version is about a factor 3.5 more efficient than the numpy-vectorized version.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The padded tf Version is about a factor {round(129./37, 1)} more efficient than the numpy-vectorized version.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_test3 = tf_N_tensor(W_DNmax, C_DNmaxK)\n",
    "tf.reduce_all(N_test3 == N_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sampling $C$**\n",
    "\n",
    "Sampling $\\Theta$ and $\\Pi$ is not very problematic because they are just dirichlet distributed (below). Sampling $C$ is sampling from \n",
    "$$\n",
    "    p(C\\vert \\Theta, \\Pi, W)=\\prod_{d=1}^D \\prod_{i=1}^{I_d} \\frac{\\prod_{k=1}^K \\left(\\pi_{dk}\\theta_{kw_{di}}\\right)^{c_{dik}}}{\\sum_{k'=1}^K\\left(\\pi_{dk'}\\theta_{k'w_{di}}\\right)}\n",
    "$$ \n",
    "which is a categorical distribution. The dependence of $I_d$ is somewhat entangled in the $w_{di}$ index of $\\Theta_{kw_{di}}$. A fully vectorized solution without loop over $D$ therefore again needs padding.\n",
    "\n",
    "https://youtu.be/z2q7LhsnWNg?t=3878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorized C-Sampling (W_DNmax must be padded!)\n",
    "@tf.function\n",
    "def sample_C(Theta, Pi, W_DNmax):\n",
    "    ## Extracting shapes\n",
    "    D = W_DNmax.shape[0]\n",
    "    N = W_DNmax.shape[1]\n",
    "    K = Pi.shape[-1]\n",
    "\n",
    "    ## Numerator\n",
    "    Theta_DKN = tf.gather(tf.transpose(Theta), W_DNmax) \n",
    "    Pi_block  = tf.stack(N * [Pi], axis=1)\n",
    "    numerator = tf.math.multiply(Pi_block, Theta_DKN)\n",
    "\n",
    "    ## Dividing by Denominator\n",
    "    denominator = tf.reduce_sum(numerator, axis=-1)\n",
    "\n",
    "    ## Sampling\n",
    "    C_DNmax_dist = tfd.Categorical(probs=numerator)\n",
    "    C_DNmax      = C_DNmax_dist.sample()\n",
    "\n",
    "    ## One-Hot-Encoding\n",
    "    C_DNmaxK = tf.one_hot(C_DNmax, K, axis=-1)\n",
    "\n",
    "    return C_DNmaxK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Padding\n",
    "#  Again using the drop-padding function as a tf.function is slow when building the graph but faster in later calls...\n",
    "@tf.function\n",
    "def drop_pad_C(C_DNmaxK, single_lengths):\n",
    "    ## Extracting shapes\n",
    "    D = C_DNmaxK.shape[0]\n",
    "    K = C_DNmaxK.shape[-1]\n",
    "\n",
    "    C_reshaped_list = []\n",
    "    for d in range(D):\n",
    "        C_reshaped_list.append(tf.reshape(C_DNmaxK[d, :single_lengths[d], :], (-1, 10)))\n",
    "\n",
    "    C_TK = tf.concat(C_reshaped_list, axis=0)\n",
    "\n",
    "    return C_TK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Tets purposes: Calculate Theta_KW in loop once.\n",
    "# Theta_KW_1 = np.zeros(shape=(K, len(W)))\n",
    "\n",
    "# Theta_temp = Theta.numpy()\n",
    "# W_temp = W.numpy()\n",
    "# for k in tqdm(range(K)):\n",
    "#     for i, w in enumerate(W_temp):\n",
    "#         Theta_KW_1[k, i] = Theta[k, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_DNmax, C_DNmaxK = padd_CW(W_T, C_TK, N_idx) # tf_padd_CW(W_T, C_TK)\n",
    "C_DNmaxK    = sample_C(Theta, Pi, W_DNmax)\n",
    "C_TK_sample = drop_pad_C(C_DNmaxK, N_idx.single_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert drop_pad_C(C_DNmaxK, N_idx.single_lengths).shape == C_TK.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sampling $\\Theta$ and $\\Pi$**\n",
    "\n",
    "Sampling $\\Theta$ and $\\Pi$:\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        p(\\Theta\\vert C, W) &= \\prod_{k=1}^K \\mathcal D(\\theta_k; \\ \\beta_{k:} + n_{\\cdot k:}) \\\\\n",
    "        p(\\Pi\\vert C, W)    &= \\prod_{d=1}^D \\mathcal D(\\pi_d; \\ \\alpha_{d:} + n_{d:\\cdot}) \\, .\n",
    "    \\end{align*}\n",
    "$$\n",
    "$\\Theta$ and $\\Pi$ are neither dependent on the number of words per document nor do they have an $I_d$ dimension. Therefore it is much easier to sample them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sample_Theta(N_DKV, beta):\n",
    "    dist_Theta = tfd.Dirichlet(beta + tf.reduce_sum(N_DKV, axis=0))\n",
    "    Theta      = dist_Theta.sample()\n",
    "    return Theta\n",
    "\n",
    "@tf.function\n",
    "def sample_Pi(N_DKV, alpha):\n",
    "    dist_Pi = tfd.Dirichlet(alpha + tf.reduce_sum(N_DKV, axis=-1))\n",
    "    Pi      = dist_Pi.sample()\n",
    "    return Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Initialization of Prior Alpha and Beta.\n",
    "#  They do not acutally matter much but could be optimized by Type II MAP or MLE.\n",
    "beta  = tf.constant(np.random.normal(size=(K, V)), dtype=tf.float32)\n",
    "alpha = tf.constant(np.random.normal(size=(D, K)), dtype=tf.float32)\n",
    "\n",
    "## Calculate one N_DKV for test purposes\n",
    "W_DNmax, C_DNmaxK = padd_CW(W_T, C_TK, N_idx)\n",
    "N_test = tf_N_tensor(W_DNmax, C_DNmaxK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Theta = sample_Theta(N_test, beta)\n",
    "test_Pi    = sample_Pi(N_test, alpha)\n",
    "\n",
    "assert test_Theta.shape == Theta.shape\n",
    "assert test_Pi.shape == Pi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Main Loop**\n",
    "\n",
    "Later: Add `@tf.function` decorator to `drop_pad_C` and replace `padd_CW` with `tf_padd_CW` in main loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_iter = 300\n",
    "\n",
    "C_T_  = tf.constant(np.random.randint(0, 10, size=(N_idx.N_total)))\n",
    "C_TK_ = tf.one_hot(C_T_, K, axis=-1)\n",
    "\n",
    "# beta  = tf.constant(np.random.normal(size=(K, V)), dtype=tf.float32)\n",
    "# alpha = tf.constant(np.random.normal(size=(D, K)), dtype=tf.float32)\n",
    "beta  = tf.zeros(shape=(K, V), dtype=tf.float32)\n",
    "alpha = tf.zeros(shape=(D, K), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `W_T` and `N_idx` are fixed since they are observable from the data and do therefore not have to be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_padd_CW = fungen_pad_CW(N_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First iteration takes a while brcause of tracing of `drop_pad_C` and `tf_pad_CW`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:13<00:00, 21.95it/s]\n"
     ]
    }
   ],
   "source": [
    "Theta_store = []\n",
    "W_DNmax, C_DNmaxK = tf_padd_CW(W_T, C_TK_)\n",
    "for iter in tqdm(range(N_iter)):\n",
    "\n",
    "    ## Padd W_T and C_TK_ to Nmax                           # One can actually indeed omit the padding reduction. Pad once and then iterate\n",
    "    # W_DNmax, C_DNmaxK = tf_padd_CW(W_T, C_TK_)            # --\n",
    "\n",
    "    ## Calculate N_DKV_ from W_DNmax and C_DNmaxK\n",
    "    N_DKV_ = tf_N_tensor(W_DNmax, C_DNmaxK)\n",
    "    \n",
    "    ## Sample Theta_ and Pi_ from N_DKV_ and priors\n",
    "    Theta_ = sample_Theta(N_DKV_, beta)\n",
    "    Pi_    = sample_Pi(N_DKV_, alpha)\n",
    "\n",
    "    ## Sample C_DNmaxK from Theta, Pi and N_DKV_\n",
    "    C_DNmaxK = sample_C(Theta_, Pi_, W_DNmax)\n",
    "\n",
    "    ## Drop Padding from C_DNmaxK                           # --\n",
    "    # C_TK_ = drop_pad_C(C_DNmaxK, N_idx.single_lengths)    # --\n",
    "\n",
    "    Theta_store.append(Theta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2be6b65f190>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.reshape(Theta_[4,:], (sqrtV, sqrtV)), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta_store = tf.stack(Theta_store, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta_mean = tf.reduce_mean(Theta_store, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2be62ffe170>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.reshape(Theta_mean[3,:], (sqrtV, sqrtV)), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "- Maybe Dropping the Padding for C is not necessairy. Maybe just collapse and expand...\n",
    "- Split `tf_padd_CW` $\\rightarrow$ Padding $W$ is necessairy just once."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tfa-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b28dcc1f87ebe436630d7ecff3a3c61833e6c22febb81b1e7cd0d34da649d2b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

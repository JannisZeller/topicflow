{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Model\n",
    "\n",
    "Trying toy model inspired by [Griffiths & Steyvers, 2004](https://doi.org/10.1073/pnas.0307752101)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "tfd = tfp.distributions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from timeit import timeit\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One hot for numpy\n",
    "def np_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sampling Behaviour of `tfp.Distributions`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 0],\n",
       "       [2, 0],\n",
       "       [2, 0],\n",
       "       [2, 0]])"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tfd.Categorical sampling behaviour:\n",
    "probs = tf.constant([[0., 0., 1.], \n",
    "                     [1., 0., 0.]])\n",
    "print(probs.shape)\n",
    "tfd.Categorical(probs=probs).sample(4).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02140409, 0.01427036, 0.9643255 ],\n",
       "       [0.38862276, 0.16176145, 0.44961584]], dtype=float32)"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tfd.Dirichlet sampling behaviour:\n",
    "conc = tf.constant([[0.1, 0.1, 0.1], \n",
    "                    [  2.,  2.,  2.]])\n",
    "tfd.Dirichlet(conc).sample().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ Samples come in by row. For Categorical the sample size is flexible. For Dirichlet the sample cise is bounded to the concentrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting up Topics and Data**\n",
    "\n",
    "### Understand The indices in this Notebook\n",
    "\n",
    "- $K$ is the number of topics \n",
    "- $D$ is the number of documents\n",
    "- $I_d$ is the number of words in document $d$\n",
    "- $N_{\\mathrm{max}}$ is the maximum number of words per doument, i. e. $N_{\\mathrm{max}} = \\max_d \\{I_d\\}$\n",
    "- $T$ is the total number of words, i. e. $T = \\sum_d I_d$\n",
    "- $V$ is the vocabulary size which should be a square of an integer for the visualization purposes of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Specify global parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constructing iterator for single documents N's\n",
    "class sliceConverter(Sequence):\n",
    "    def __init__(self, N_D: tf.Tensor):\n",
    "        assert N_D.ndim == 1\n",
    "        N_idx = tf.cumsum(N_D)\n",
    "        N_idx = tf.repeat(N_idx, 2)\n",
    "        self.N_idx   = tf.concat([[0], N_idx], axis=0).numpy()\n",
    "        self.N_max   = tf.reduce_mean(N_D).numpy()\n",
    "        self.N_total = N_idx[-1].numpy()\n",
    "        self.single_lengths = N_D\n",
    "\n",
    "    def __len__(self):\n",
    "        return int((self.N_idx.shape[0] - 1) / 2)\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        # if type(k) == int:\n",
    "        #     if k >= 0:\n",
    "        #         return tf.range(self.N_idx[2*k], self.N_idx[2*k + 1])\n",
    "        #     if k < 0:\n",
    "        #         i = len(self) + k\n",
    "        #         return tf.range(self.N_idx[2*i], self.N_idx[2*i + 1])\n",
    "        # elif type(k) == slice:\n",
    "        #     start, stop, step = k.indices(len(self))\n",
    "        #     return [self[i] for i in range(start, stop, step)]\n",
    "        if type(k) == int:\n",
    "            if k >= 0:\n",
    "                return slice(self.N_idx[2*k], self.N_idx[2*k + 1], 1)\n",
    "            if k < 0:\n",
    "                i = len(self) + k\n",
    "                return slice(self.N_idx[2*i], self.N_idx[2*i + 1], 1)\n",
    "        elif type(k) == slice:\n",
    "            start, stop, step = k.indices(len(self))\n",
    "            return [self[i] for i in range(start, stop, step)]\n",
    "        else:\n",
    "            raise TypeError(\"Index must be integer or slice.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.idx = 0\n",
    "        self.maxiter = len(self)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.idx < self.maxiter:\n",
    "            ret = self.__getitem__(self.idx)\n",
    "            self.idx += 1\n",
    "            return ret\n",
    "        else:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "K       = 10    # Number of Topics\n",
    "sqrtV   = 5     # Square Root of the Number of \"Vocabulary\" (must be sqrt such that pictorial interpretation is possible)\n",
    "D       = 1000  # Number of documents\n",
    "N_rate  = None   # Number of words per document (either as rate for Poisson Distribution\n",
    "N_fixed = 100  #   or as fixed value)\n",
    "\n",
    "V = int(sqrtV**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Generating Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_LDA(D: int, K: int, sqrtV: int, N_fixed: int=None, N_rate: int=None) -> tuple():\n",
    "    \"\"\"\n",
    "    Generates LDA-Data from the LDA generative Process. \n",
    "    Thetas get constructed as rows and columns of a quadratic sqrtV-grid.\n",
    "    Either N_fixed ot N_rate has to be passed\n",
    "    \"\"\"\n",
    "    assert (N_fixed is None) != (N_rate is None), \"Exactly one of N_fixed and N_rate must be passed.\"\n",
    "\n",
    "\n",
    "    ## Number of words per document\n",
    "    if N_rate is not None:\n",
    "        N_D_dist = tfd.Poisson(rate=100)\n",
    "        N_D      = tf.cast(N_D_dist.sample(D), dtype=tf.int32)\n",
    "    if N_fixed is not None:\n",
    "        N_D = np.array(D*[N_fixed])\n",
    "    N_idx    = sliceConverter(N_D)\n",
    "    \n",
    "\n",
    "    ## Word grid\n",
    "    V = int(sqrtV**2)\n",
    "    V_grid = np.reshape(np.arange(0, V), newshape=(sqrtV, sqrtV))\n",
    "\n",
    "\n",
    "    ## Topic-Word Distribution\n",
    "    #  Words belonging to a topic are rows and columns\n",
    "    Theta_idx = [row for row in V_grid] + [col for col in V_grid.T]\n",
    "    Theta = np.zeros((K, V))\n",
    "    for k, idx in enumerate(Theta_idx):\n",
    "        Theta[k, idx] = 1. / sqrtV\n",
    "    Theta = tf.constant(Theta, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    ## Document-Topic Distribution\n",
    "    #  Initializing Alpha manually to 1.\n",
    "    Alpha = 1\n",
    "    dist_Pi = tfd.Dirichlet(K*[Alpha])\n",
    "    Pi      = dist_Pi.sample(D)\n",
    "\n",
    "\n",
    "    ## Topic Assignments of word c_{dik} of word w_{di}\n",
    "    dist_C  = tfd.Categorical(probs=Pi)\n",
    "    C_NmaxD = dist_C.sample(tf.reduce_max(N_D))\n",
    "\n",
    "    C_T = []\n",
    "    for d in range(D):\n",
    "        C_T_col = C_NmaxD[:N_D[d], d]\n",
    "        C_T.append(C_T_col)\n",
    "    C_T = tf.concat(C_T, axis=0)\n",
    "    C_TK = tf.one_hot(C_T, depth=K, axis=-1)\n",
    "\n",
    "\n",
    "    ## Draw words w_{di}\n",
    "    dist_W_T = tfd.Categorical(tf.gather(Theta, C_T))\n",
    "    W_T      = dist_W_T.sample()\n",
    "\n",
    "\n",
    "    return Theta, Pi, C_T, C_TK, W_T, N_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta, Pi, C_T, C_TK, W_T, N_idx = generate_LDA(D, K, sqrtV, N_fixed=N_fixed, N_rate=N_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics for the Words in Document d=11:\n",
      "tf.Tensor(\n",
      "[1 6 1 5 8 8 8 6 4 1 6 4 5 3 6 5 9 6 4 5 2 3 5 5 1 5 9 4 5 4 8 6 1 4 9 8 3\n",
      " 5 5 8 3 5 8 5 6 5 3 9 1 9 4 6 6 8 9 5 4 6 4 3 5 9 6 7 9 8 5 2 4 4 7 9 9 4\n",
      " 5 7 6 5 5 8 3 9 4 5 8 6 8 3 5 4 5 7 9 3 5 6 5 6 3 4], shape=(100,), dtype=int32)\n",
      "\n",
      "Words in Document d=11:\n",
      "tf.Tensor(\n",
      "[20 13 11 19  3 18 14  7 12 14  6 19  1  1  7 13 18 21 10  5 23  4  0 15\n",
      " 18  4  9 11  5 19 18 18  9 20  3 21 12 17 19 23 17  1 10 11 19 16  2 17\n",
      " 19  6  6 21  2 16 11  6  8 18 15 17  2  6 16  8  0 23  4  1 13  5  7 19\n",
      " 17 23 20 18 21  0  3 15 19 11  8 20  8 21  7 20 13  0  9 17 12  7 10 21\n",
      " 13  8 16  5], shape=(100,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "## To extract topic-word assignments and document-words for a \n",
    "#  specific document, use the slice converter:\n",
    "doc_slice = N_idx[11]\n",
    "\n",
    "print(\"Topics for the Words in Document d=11:\")\n",
    "print(C_T[doc_slice])\n",
    "print(\"\\nWords in Document d=11:\")\n",
    "print(W_T[doc_slice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For development purposes:\n",
    "def print_all_shapes():\n",
    "    print(\"Π-Shape (D docs x K topics)\")\n",
    "    print(Pi.shape)\n",
    "    print(\"\\nΘ-Shape (K topic x V vocab)\")\n",
    "    print(Theta.shape)\n",
    "    print(\"\\nC_T-Shape (Topic for each word T=sum_d I_d)\")\n",
    "    print(C_T.shape)\n",
    "    print(\"\\nC_TK-Shape (one-hot for each topic k in {1, K})\")\n",
    "    print(C_TK.shape)\n",
    "    print(\"\\nW-Shape (sum_d I_d words)\")\n",
    "    print(W_T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Checking Shapes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Π-Shape (D docs x K topics)\n",
      "(1000, 10)\n",
      "\n",
      "Θ-Shape (K topic x V vocab)\n",
      "(10, 25)\n",
      "\n",
      "C_T-Shape (Topic for each word T=sum_d I_d)\n",
      "(100000,)\n",
      "\n",
      "C_TK-Shape (one-hot for each topic k in {1, K})\n",
      "(100000, 10)\n",
      "\n",
      "W-Shape (sum_d I_d words)\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "print_all_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_to_img(arr, sqrtV=5):\n",
    "    vals = dict(zip(*np.unique(arr, return_counts=True)))\n",
    "    img = []\n",
    "\n",
    "    for i in range(int(sqrtV**2)):\n",
    "        if i in vals:\n",
    "            img.append(vals[i])\n",
    "        else:\n",
    "            img.append(0)\n",
    "\n",
    "    img = np.array(img).reshape(sqrtV, sqrtV)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([0.38780424, 0.18907806, 0.02911452, 0.06413591, 0.19551848,\n",
       "       0.0772825 , 0.03335454, 0.00486895, 0.01072818, 0.00811451],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pi[10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x177f29b1c30>"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(format_to_img(W_T[N_idx[10]]), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x177f2a5b4f0>"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.reshape(Theta[0,:], (sqrtV, sqrtV)), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gibbs Sampler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **N-Tensor**\n",
    "One efficiency crtitical step is to vectorize \n",
    "$$\n",
    "    n_{dkv} =  \\{i \\, \\vert \\, w_{di} == v \\ \\& \\ c_{idk} ==1\\}\n",
    "$$\n",
    "as much as possible. Due to the fact, that the document lengths are variable there is the choice between looping over the number of documents or padding the documents to a unique length. The padding does not need a word-token, it can be done by padding the $C$ matrix with $0$, therefore it will not affect $N$. Then the counting can be realized by stacking $W$ $K$ times along the last axis to match the shape of $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Multiloop over all Dimensions\n",
    "\n",
    "Mainly for test purposes. Horribly slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiloop_N_tensor(W_T, C_TK, N_idx, D=1000, V=25):\n",
    "    K    = C_TK.shape[1]\n",
    "    W_T  = W_T.numpy()\n",
    "    C_TK = C_TK.numpy()\n",
    "    N_DKV = np.zeros(shape=(D, K, V))\n",
    "    \n",
    "    for d in tqdm(range(D)):\n",
    "        di = N_idx[d]\n",
    "        for k in range(K):\n",
    "            for v in range(V):\n",
    "                for w_di, c_dik in zip(W_T[di], C_TK[di, k]):\n",
    "                    if w_di == v and c_dik == 1:\n",
    "                        N_DKV[d, k, v] += 1\n",
    "\n",
    "    return N_DKV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test and Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_test1 = multiloop_N_tensor(W_T, C_TK, N_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit multiloop_N_tensor(W_T, C_TK, N_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Single-Loop over $D$ with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleloop_N_tensor(W_T, C_TK, N_idx, D=1000, V=25, verbose=False):\n",
    "    K = C_TK.shape[1]\n",
    "    W_T   = W_T.numpy()\n",
    "    C_TK  = C_TK.numpy()\n",
    "    N_DKV = np.zeros(shape=(D, K, V))\n",
    "\n",
    "    ## Preparing W-stacking by shifting all entries one \"up\" s. t. v is counted \n",
    "    #  from 1 to 25 instead from 0 to 24. This enables to collapse the \"&\" in the\n",
    "    #  set to be collapsed to a matrix product\n",
    "    Wp1 = W_T + 1\n",
    "    \n",
    "    if verbose:\n",
    "        iter = tqdm(range(D))  \n",
    "    if not verbose:\n",
    "        iter = range(D)\n",
    "\n",
    "    for d in iter:\n",
    "        di = N_idx[d]\n",
    "        W_ = Wp1[di]\n",
    "        C_TK_ = C_TK[di]\n",
    "\n",
    "        Nd = len(W_)\n",
    "\n",
    "        ## Stacking W\n",
    "        W_stacked = np.stack(K*[W_], axis=-1)\n",
    "\n",
    "        ## Elementwise product combines logical & in conditio\n",
    "        #  Choosing int32 as product dtype for efficiency.\n",
    "        C_Dot_W = np.multiply(W_stacked, C_TK_)\n",
    "\n",
    "        ## The v-dimension of N is a one-hot encoding for the vocabulary:\n",
    "        N_dNdKVp1 = np_one_hot(C_Dot_W.astype(np.int32), V+1)\n",
    "\n",
    "        ## Reverting the v-shift by dropping the 0 one-hot dimension\n",
    "        N_dNdKV = N_dNdKVp1[:, :, 1:]\n",
    "\n",
    "        ## Summing along v-dimension\n",
    "        assert N_dNdKV.shape[0] == Nd\n",
    "        N_dKV = np.sum(N_dNdKV, axis=0)\n",
    "\n",
    "        ## Append to full tensor\n",
    "        N_DKV[d, :, :] = N_dKV\n",
    "\n",
    "    return N_DKV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test and Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test2 = singleloop_N_tensor(W_T, C_TK, N_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit singleloop_N_tensor(W, C_TK, N_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy implementation which avoides all loops but the loop over documents is about a factor 63.2 more efficient as multiloop version.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numpy implementation which avoides all loops but the loop over documents is about a factor {round(8150./129., 1)} more efficient as multiloop version.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Vectorized with Padding using `tf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding Functions\n",
    "def fungen_pad_CW(N_idx):\n",
    "    \n",
    "    @tf.function\n",
    "    def padd_CW(W_T, C_TK):\n",
    "        K = C_TK.shape[1]\n",
    "\n",
    "        ## Reshaping C_TK to C_DNmaxK and\n",
    "        #            W_T  to W_DNmax\n",
    "        C_DNmaxK_list = []\n",
    "        W_DNmax_list  = []\n",
    "        N_max = tf.reduce_max(N_idx.single_lengths)\n",
    "\n",
    "        for di in N_idx:\n",
    "            CdiK = C_TK[di, :]\n",
    "            Wdi  = W_T[di]\n",
    "            Nd   = CdiK.shape[0]\n",
    "\n",
    "            N_pad = N_max - Nd\n",
    "\n",
    "            ## C\n",
    "            pad = tf.zeros(shape=(N_pad, K), dtype=CdiK.dtype)\n",
    "            CdiK_pad = tf.concat([CdiK, pad], axis=0)\n",
    "            C_DNmaxK_list.append(CdiK_pad)\n",
    "\n",
    "            ## W\n",
    "            pad = tf.zeros(shape=(N_pad), dtype=Wdi.dtype)\n",
    "            Wdi_pad = tf.concat([Wdi, pad], axis=0)\n",
    "            W_DNmax_list.append(Wdi_pad)\n",
    "\n",
    "        C_DNmaxK = tf.stack(C_DNmaxK_list, axis=0)\n",
    "        W_DNmax  = tf.stack(W_DNmax_list, axis=0)\n",
    "\n",
    "        return W_DNmax, C_DNmaxK\n",
    "\n",
    "    return padd_CW\n",
    "\n",
    "## Using a tf.function decorator causes the creation to take pretty long because a graph\n",
    "#  is built which has to take the document lengths into account. This might be only \n",
    "#  useful if the function itself takes long to run without a tf.function... Maybe\n",
    "#  Better use just the plain-non tf.function-converted:\n",
    "\n",
    "def padd_CW(W_T, C_TK, N_idx):\n",
    "    K = C_TK.shape[1]\n",
    "\n",
    "    ## Reshaping C_TK to C_DNmaxK and\n",
    "    #            W_T  to W_DNmax\n",
    "    C_DNmaxK_list = []\n",
    "    W_DNmax_list  = []\n",
    "    N_max = tf.reduce_max(N_idx.single_lengths)\n",
    "\n",
    "    for di in N_idx:\n",
    "        CdiK = C_TK[di, :]\n",
    "        Wdi  = W_T[di]\n",
    "        Nd   = CdiK.shape[0]\n",
    "\n",
    "        N_pad = N_max - Nd\n",
    "\n",
    "        ## C\n",
    "        pad = tf.zeros(shape=(N_pad, K), dtype=CdiK.dtype)\n",
    "        CdiK_pad = tf.concat([CdiK, pad], axis=0)\n",
    "        C_DNmaxK_list.append(CdiK_pad)\n",
    "\n",
    "        ## W\n",
    "        pad = tf.zeros(shape=(N_pad), dtype=Wdi.dtype)\n",
    "        Wdi_pad = tf.concat([Wdi, pad], axis=0)\n",
    "        W_DNmax_list.append(Wdi_pad)\n",
    "\n",
    "    C_DNmaxK = tf.stack(C_DNmaxK_list, axis=0)\n",
    "    W_DNmax  = tf.stack(W_DNmax_list, axis=0)\n",
    "\n",
    "    return W_DNmax, C_DNmaxK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Actual n-Tensor\n",
    "@tf.function\n",
    "def tf_N_tensor(W_DNmax, C_DNmaxK, V=25):\n",
    "\n",
    "    ## Extracting shapes\n",
    "    D = C_DNmaxK.shape[0]\n",
    "    N = C_DNmaxK.shape[1]\n",
    "    K = C_DNmaxK.shape[2]\n",
    "    \n",
    "    ## Preparing W-stacking by shifting all entries one \"up\" s. t. v is counted \n",
    "    #  from 1 to 25 instead from 0 to 24. This enables to collapse the \"&\" in the\n",
    "    #  set to be collapsed to a matrix product\n",
    "    Wp1 = W_DNmax + 1\n",
    "    W_stacked = tf.stack(K*[Wp1], axis=-1)    \n",
    "\n",
    "    ## Elementwise product combines logical & in condition.\n",
    "    #  Choosing int32 as product dtype for efficiency.\n",
    "    C_DNmaxK_int = tf.cast(C_DNmaxK, dtype=tf.int32)\n",
    "    C_Dot_W = tf.math.multiply(W_stacked, C_DNmaxK_int)\n",
    "\n",
    "    ## The v-dimension of N is a one-hot encoding for the vocabulary:\n",
    "    N_DNKVp1 = tf.one_hot(C_Dot_W, V+1, dtype=tf.int32)\n",
    "\n",
    "    ## Reverting the v-shift by dropping the 0 one-hot dimension\n",
    "    N_DNKV = N_DNKVp1[:, :, :, 1:]\n",
    "\n",
    "    ## Summing along v-dimension\n",
    "    assert N_DNKV.shape[1] == N\n",
    "    N_DKV = tf.reduce_sum(N_DNKV, axis=1)\n",
    "\n",
    "    ## Turn to float for gibbs sampler\n",
    "    N_DKV = tf.cast(N_DKV, dtype=tf.float32)\n",
    "\n",
    "    return N_DKV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test and Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit tf_padd_CW(W, C_TK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit padd_CW(W, C_TK, N_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a tf.function the padding is about a factor 15.4 more efficient, when the graph is already constructed. Yet constructing the graph takes a while.\n"
     ]
    }
   ],
   "source": [
    "print(f\"With a tf.function the padding is about a factor {round(970./63., 1)} more efficient, when the graph is already constructed. Yet constructing the graph takes a while.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_DNmax, C_DNmaxK = padd_CW(W_T, C_TK, N_idx)\n",
    "# tf_padd_CW = fungen_pad_CW(N_idx)\n",
    "# W_DNmax, C_DNmaxK = tf_padd_CW(W_T, C_TK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Tensor itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit tf_N_tensor(W_DNmax, C_DNmaxK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The padded tf Version is about a factor 3.5 more efficient than the numpy-vectorized version.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The padded tf Version is about a factor {round(129./37, 1)} more efficient than the numpy-vectorized version.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_test3 = tf_N_tensor(W_DNmax, C_DNmaxK)\n",
    "tf.reduce_all(N_test3 == N_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sampling $C$**\n",
    "\n",
    "Sampling $\\Theta$ and $\\Pi$ is not very problematic because they are just dirichlet distributed (below). Sampling $C$ is sampling from \n",
    "$$\n",
    "    p(C\\vert \\Theta, \\Pi, W)=\\prod_{d=1}^D \\prod_{i=1}^{I_d} \\frac{\\prod_{k=1}^K \\left(\\pi_{dk}\\theta_{kw_{di}}\\right)^{c_{dik}}}{\\sum_{k'=1}^K\\left(\\pi_{dk'}\\theta_{k'w_{di}}\\right)}\n",
    "$$ \n",
    "which is a categorical distribution. The dependence of $I_d$ is somewhat entangled in the $w_{di}$ index of $\\Theta_{kw_{di}}$. A fully vectorized solution without loop over $D$ therefore again needs padding.\n",
    "\n",
    "https://youtu.be/z2q7LhsnWNg?t=3878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorized C-Sampling (W_DNmax must be padded!)\n",
    "@tf.function\n",
    "def sample_C(Theta, Pi, W_DNmax):\n",
    "    ## Extracting shapes\n",
    "    D = W_DNmax.shape[0]\n",
    "    N = W_DNmax.shape[1]\n",
    "    K = Pi.shape[-1]\n",
    "\n",
    "    ## Numerator\n",
    "    Theta_KDNmax = tf.gather(Theta, W_DNmax, axis=-1) \n",
    "    Pi_block = tf.stack(Theta_KDNmax.shape[2]*[tf.transpose(Pi)], axis=-1)\n",
    "    numerator = tf.math.multiply(Pi_block, Theta_KDNmax)\n",
    "    numerator = tf.reshape(numerator, (D, N, K))\n",
    "\n",
    "    ## Dividing by Denominator\n",
    "    denominator = tf.reduce_sum(numerator, axis=-1)\n",
    "    numerator = numerator / tf.expand_dims(denominator, axis=-1)\n",
    "\n",
    "    ## Sampling\n",
    "    C_DNmax_dist = tfd.Categorical(numerator)\n",
    "    C_DNmax      = C_DNmax_dist.sample()\n",
    "\n",
    "    ## One-Hot-Encoding\n",
    "    C_DNmaxK = tf.one_hot(C_DNmax, K, axis=-1)\n",
    "\n",
    "    return C_DNmaxK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Padding\n",
    "#  Again using the drop-padding function as a tf.function is slow when building the graph but faster in later calls...\n",
    "@tf.function\n",
    "def drop_pad_C(C_DNmaxK, single_lengths):\n",
    "    ## Extracting shapes\n",
    "    D = C_DNmaxK.shape[0]\n",
    "    K = C_DNmaxK.shape[-1]\n",
    "\n",
    "    C_reshaped_list = []\n",
    "    for d in range(D):\n",
    "        C_reshaped_list.append(tf.reshape(C_DNmaxK[d, :single_lengths[d], :], (-1, 10)))\n",
    "\n",
    "    C_TK = tf.concat(C_reshaped_list, axis=0)\n",
    "\n",
    "    return C_TK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Tets purposes: Calculate Theta_KW in loop once.\n",
    "# Theta_KW_1 = np.zeros(shape=(K, len(W)))\n",
    "\n",
    "# Theta_temp = Theta.numpy()\n",
    "# W_temp = W.numpy()\n",
    "# for k in tqdm(range(K)):\n",
    "#     for i, w in enumerate(W_temp):\n",
    "#         Theta_KW_1[k, i] = Theta[k, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_DNmax, C_DNmaxK = padd_CW(W_T, C_TK, N_idx) # tf_padd_CW(W_T, C_TK)\n",
    "C_DNmaxK    = sample_C(Theta, Pi, W_DNmax)\n",
    "C_TK_sample = drop_pad_C(C_DNmaxK, N_idx.single_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert drop_pad_C(C_DNmaxK, N_idx.single_lengths).shape == C_TK.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sampling $\\Theta$ and $\\Pi$**\n",
    "\n",
    "Sampling $\\Theta$ and $\\Pi$:\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        p(\\Theta\\vert C, W) &= \\prod_{k=1}^K \\mathcal D(\\theta_k; \\ \\beta_{k:} + n_{\\cdot k:}) \\\\\n",
    "        p(\\Pi\\vert C, W)    &= \\prod_{d=1}^D \\mathcal D(\\pi_d; \\ \\alpha_{d:} + n_{d:\\cdot}) \\, .\n",
    "    \\end{align*}\n",
    "$$\n",
    "$\\Theta$ and $\\Pi$ are neither dependent on the number of words per document nor do they have an $I_d$ dimension. Therefore it is much easier to sample them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sample_Theta(N_DKV, beta):\n",
    "    dist_Theta = tfd.Dirichlet(beta + tf.reduce_sum(N_DKV, axis=0))\n",
    "    Theta      = dist_Theta.sample()\n",
    "    return Theta\n",
    "\n",
    "@tf.function\n",
    "def sample_Pi(N_DKV, alpha):\n",
    "    dist_Pi = tfd.Dirichlet(alpha + tf.reduce_sum(N_DKV, axis=-1))\n",
    "    Pi      = dist_Pi.sample()\n",
    "    return Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Initialization of Prior Alpha and Beta.\n",
    "#  They do not acutally matter much but could be optimized by Type II MAP or MLE.\n",
    "beta  = tf.constant(np.random.normal(size=(K, V)), dtype=tf.float32)\n",
    "alpha = tf.constant(np.random.normal(size=(D, K)), dtype=tf.float32)\n",
    "\n",
    "## Calculate one N_DKV for test purposes\n",
    "W_DNmax, C_DNmaxK = padd_CW(W_T, C_TK, N_idx)\n",
    "N_test = tf_N_tensor(W_DNmax, C_DNmaxK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Theta = sample_Theta(N_test, beta)\n",
    "test_Pi    = sample_Pi(N_test, alpha)\n",
    "\n",
    "assert test_Theta.shape == Theta.shape\n",
    "assert test_Pi.shape == Pi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Main Loop**\n",
    "\n",
    "Later: Add `@tf.function` decorator to `drop_pad_C` and replace `padd_CW` with `tf_padd_CW` in main loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_iter = 200\n",
    "\n",
    "C_T_  = tf.constant(np.random.randint(0, 10, size=(N_idx.N_total)))\n",
    "C_TK_ = tf.one_hot(C_T_, K, axis=-1)\n",
    "\n",
    "# beta  = tf.constant(np.random.normal(size=(K, V)), dtype=tf.float32)\n",
    "# alpha = tf.constant(np.random.normal(size=(D, K)), dtype=tf.float32)\n",
    "beta  = tf.zeros(shape=(K, V), dtype=tf.float32)\n",
    "alpha = tf.zeros(shape=(D, K), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `W_T` and `N_idx` are fixed since they are observable from the data and do therefore not have to be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_padd_CW = fungen_pad_CW(N_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First iteration takes a while brcause of tracing of `drop_pad_C` and `tf_pad_CW`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:44<00:00,  4.54it/s]\n"
     ]
    }
   ],
   "source": [
    "Theta_store = []\n",
    "\n",
    "for iter in tqdm(range(N_iter)):\n",
    "\n",
    "    ## Padd W_T and C_TK_ to Nmax\n",
    "    W_DNmax, C_DNmaxK = tf_padd_CW(W_T, C_TK_)\n",
    "\n",
    "    ## Calculate N_DKV_ from W_DNmax and C_DNmaxK\n",
    "    N_DKV_ = tf_N_tensor(W_DNmax, C_DNmaxK)\n",
    "    \n",
    "    ## Sample Theta_ and Pi_ from N_DKV_ and priors\n",
    "    Theta_ = sample_Theta(N_DKV_, beta)\n",
    "    Pi_    = sample_Pi(N_DKV_, alpha)\n",
    "\n",
    "    ## Sample C_DNmaxK from Theta, Pi and N_DKV_\n",
    "    C_DNmaxK = sample_C(Theta_, Pi_, W_DNmax)\n",
    "\n",
    "    ## Drop Padding from C_DNmaxK\n",
    "    C_TK_ = drop_pad_C(C_DNmaxK, N_idx.single_lengths)\n",
    "\n",
    "    Theta_store.append(Theta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1781be8fb50>"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.reshape(Theta_[3,:], (sqrtV, sqrtV)), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta_store = tf.stack(Theta_store, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta_mean = tf.reduce_mean(Theta_store, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1781b625c30>"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.reshape(Theta_mean[3,:], (sqrtV, sqrtV)), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "- Maybe Dropping the Padding for C is not necessairy. Maybe just collapse and expand...\n",
    "- Split `tf_padd_CW` $\\rightarrow$ Padding $W$ is necessairy just once."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tfa-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b28dcc1f87ebe436630d7ecff3a3c61833e6c22febb81b1e7cd0d34da649d2b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Model\n",
    "\n",
    "Trying toy model inspired by [Griffiths & Steyvers, 2004](https://doi.org/10.1073/pnas.0307752101)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "tfd = tfp.distributions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from timeit import timeit\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One hot for numpy\n",
    "def np_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Behaviour of `tfp.Distributions`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 0],\n",
       "       [2, 0],\n",
       "       [2, 0],\n",
       "       [2, 0]])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tfd.Categorical sampling behaviour:\n",
    "probs = tf.constant([[0., 0., 1.], \n",
    "                     [1., 0., 0.]])\n",
    "print(probs.shape)\n",
    "tfd.Categorical(probs=probs).sample(4).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02140409, 0.01427036, 0.9643255 ],\n",
       "       [0.38862276, 0.16176145, 0.44961584]], dtype=float32)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tfd.Dirichlet sampling behavious:\n",
    "conc = tf.constant([[0.1, 0.1, 0.1], \n",
    "                    [  2.,  2.,  2.]])\n",
    "tfd.Dirichlet(conc).sample().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ Samples come in by row. For Categorical the sample size is flexible. For Dirichlet the sample cise is bounded to the concentrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting up Topics and Data**\n",
    "\n",
    "1. Specify global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constructing iterator for single documents N's\n",
    "class sliceConverter(Sequence):\n",
    "    def __init__(self, N_D: tf.Tensor):\n",
    "        assert N_D.ndim == 1\n",
    "        N_idx = tf.cumsum(N_D)\n",
    "        N_idx = tf.repeat(N_idx, 2)\n",
    "        self.N_idx = tf.concat([[0], N_idx], axis=0).numpy()\n",
    "        self.single_lengths = N_D\n",
    "\n",
    "    def __len__(self):\n",
    "        return int((self.N_idx.shape[0] - 1) / 2)\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        # if type(k) == int:\n",
    "        #     if k >= 0:\n",
    "        #         return tf.range(self.N_idx[2*k], self.N_idx[2*k + 1])\n",
    "        #     if k < 0:\n",
    "        #         i = len(self) + k\n",
    "        #         return tf.range(self.N_idx[2*i], self.N_idx[2*i + 1])\n",
    "        # elif type(k) == slice:\n",
    "        #     start, stop, step = k.indices(len(self))\n",
    "        #     return [self[i] for i in range(start, stop, step)]\n",
    "        if type(k) == int:\n",
    "            if k >= 0:\n",
    "                return slice(self.N_idx[2*k], self.N_idx[2*k + 1], 1)\n",
    "            if k < 0:\n",
    "                i = len(self) + k\n",
    "                return slice(self.N_idx[2*i], self.N_idx[2*i + 1], 1)\n",
    "        elif type(k) == slice:\n",
    "            start, stop, step = k.indices(len(self))\n",
    "            return [self[i] for i in range(start, stop, step)]\n",
    "        else:\n",
    "            raise TypeError(\"Index must be integer or slice.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.idx = 0\n",
    "        self.maxiter = len(self)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.idx < self.maxiter:\n",
    "            ret = self.__getitem__(self.idx)\n",
    "            self.idx += 1\n",
    "            return ret\n",
    "        else:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of Topics\n",
    "K = 10\n",
    "\n",
    "## Square Root of the Number of \"Vocabulary\" (must be sqrt such that pictorial interpretation is possible)\n",
    "sqrtV = 5\n",
    "\n",
    "## Number of documents\n",
    "D = 1000\n",
    "\n",
    "## Number of words per document\n",
    "N_D_dist = tfd.Poisson(rate=100)\n",
    "N_D     = tf.cast(N_D_dist.sample(D), dtype=tf.int32)\n",
    "N_total = tf.reduce_sum(N_D)\n",
    "N_max   = tf.reduce_max(N_D)\n",
    "N_idx = sliceConverter(N_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 2.5))\n",
    "plt.hist(N_D)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word grid\n",
    "V = int(sqrtV**2)\n",
    "V_grid = np.reshape(np.arange(0, V), newshape=(sqrtV, sqrtV))\n",
    "\n",
    "## Topic-Word Distribution\n",
    "#  Words belonging to a topic are rows and columns\n",
    "Theta_idx = [row for row in V_grid] + [col for col in V_grid.T]\n",
    "Theta = np.zeros((K, V))\n",
    "for k, idx in enumerate(Theta_idx):\n",
    "    Theta[k, idx] = 1. / sqrtV\n",
    "Theta = tf.constant(Theta, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Document-Topic Distribution\n",
    "Alpha = 1\n",
    "dist_Pi = tfd.Dirichlet(K*[Alpha])\n",
    "Pi      = dist_Pi.sample(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Topic Assignments of word c_{dik} of word w_{di}\n",
    "dist_C  = tfd.Categorical(probs=Pi)\n",
    "C_NmaxD = dist_C.sample(N_max)\n",
    "\n",
    "C = []\n",
    "for d in range(D):\n",
    "    C_col = C_NmaxD[:N_D[d], d]\n",
    "    C.append(C_col)\n",
    "\n",
    "C = tf.concat(C, axis=0)\n",
    "assert C.ndim == 1 and C.shape[0] == N_total\n",
    "\n",
    "C_one_hot = tf.one_hot(C, depth=K, axis=-1)\n",
    "assert tf.reduce_all(tf.reduce_sum(C_one_hot, axis=-1) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Draw words w_{di}\n",
    "dist_W = tfd.Categorical(tf.gather(Theta, C))\n",
    "W = dist_W.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics for the Words in Document d=11:\n",
      "tf.Tensor(\n",
      "[8 8 1 8 8 8 3 8 1 1 8 4 0 6 3 1 9 8 8 4 8 1 9 1 7 3 0 3 8 8 4 3 8 9 3 9 6\n",
      " 1 3 8 9 2 3 8 3 8 8 5 9 6 3 1 5 0 0 9 3 1 1 4 3 8 0 9 5 8 1 0 9 0 1 1 3 8\n",
      " 3 3 8 3 0 3 0 7 3 3 0 8 9 8 0 5 9 3 3 7 5 2 0 8 5 0 1 3 5 1 5 0 5 8 5 8 3\n",
      " 9 8 3 8 8 8 3 0 6 3], shape=(121,), dtype=int32)\n",
      "\n",
      "Words in Document d=11:\n",
      "tf.Tensor(\n",
      "[ 1  7  8 12  6  6 23 23 10  1 16 21 18 14 21 10  7  3 15 15  8 22 22 12\n",
      " 24 18 13  8 13 20  9 18 15 22  0 21  0 10 22 15 10 12  0  2 16  2 23 10\n",
      "  5 21 21 16  8 16  6 21  6 14  1 18  3 22 17 19  0  1  6  3 16 15 10 16\n",
      " 21 19  5 20 24 17 11 13 21  8 18 13 15 11  4 14 19 15 21 10 23 24  8 22\n",
      " 17  2  0 19  5  6  7 23  2  0 20  9 16 14  5 11  4 11  5  3  0 14  4 19\n",
      " 16], shape=(121,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "## To extract topic-word assignments and document-words for a \n",
    "#  specific document, use the slice converter:\n",
    "doc_slice = N_idx[11]\n",
    "\n",
    "print(\"Topics for the Words in Document d=11:\")\n",
    "print(C[doc_slice])\n",
    "print(\"\\nWords in Document d=11:\")\n",
    "print(W[doc_slice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For development purposes:\n",
    "def print_all_shapes():\n",
    "    print(\"Π-Shape\")\n",
    "    print(Pi.shape)\n",
    "    print(\"\\nΘ-Shape\")\n",
    "    print(Theta.shape)\n",
    "    print(\"\\nC-Shape\")\n",
    "    print(C.shape)\n",
    "    print(\"\\nC_one_hot-Shape\")\n",
    "    print(C_one_hot.shape)\n",
    "    print(\"\\nW-Shape\")\n",
    "    print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_to_img(arr, sqrtV=5):\n",
    "    vals = dict(zip(*np.unique(arr, return_counts=True)))\n",
    "    img = []\n",
    "\n",
    "    for i in range(V):\n",
    "        if i in vals:\n",
    "            img.append(vals[i])\n",
    "        else:\n",
    "            img.append(0)\n",
    "\n",
    "    img = np.array(img).reshape(sqrtV, sqrtV)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1966e7f1b40>"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(format_to_img(W[N_idx[10]]), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Tensor\n",
    "One efficiency crtitical step is to vectorize \n",
    "$$\n",
    "    n_{dkv} =  \\{i \\, \\vert \\, w_{di} == v \\ \\& \\ c_{idk} ==1\\}\n",
    "$$\n",
    "as much as possible. Due to the fact, that the document lengths are variable there is the choice between looping over the number of documents or padding the documents to a unique length. The padding does not need a word-token, it can be done by padding the $C$ matrix with $0$, therefore it will not affect $N$. Then the counting can be realized by stacking $W$ $K$ times along the last axis to match the shape of $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiloop_N_tensor(W, C_one_hot, N_idx, D=1000, V=25):\n",
    "    K = C_one_hot.shape[1]\n",
    "    W         = W.numpy()\n",
    "    C_one_hot = C_one_hot.numpy()\n",
    "    N_DKV = np.zeros(shape=(D, K, V))\n",
    "    \n",
    "    for d in tqdm(range(D)):\n",
    "        di = N_idx[d]\n",
    "        for k in range(K):\n",
    "            for v in range(V):\n",
    "                for w_di, c_dik in zip(W[di], C_one_hot[di, k]):\n",
    "                    if w_di == v and c_dik == 1:\n",
    "                        N_DKV[d, k, v] += 1\n",
    "\n",
    "    return N_DKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_test1 = multiloop_N_tensor(W, C_one_hot, N_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit multiloop_N_tensor(W, C_one_hot, N_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleloop_N_tensor(W, C_one_hot, N_idx, D=1000, V=25, verbose=False):\n",
    "    K = C_one_hot.shape[1]\n",
    "    W         = W.numpy()\n",
    "    C_one_hot = C_one_hot.numpy()\n",
    "    N_DKV = np.zeros(shape=(D, K, V))\n",
    "\n",
    "    ## Preparing W-stacking by shifting all entries one \"up\" s. t. v is counted \n",
    "    #  from 1 to 25 instead from 0 to 24. This enables to collapse the \"&\" in the\n",
    "    #  set to be collapsed to a matrix product\n",
    "    Wp1 = W + 1\n",
    "    \n",
    "    if verbose:\n",
    "        iter = tqdm(range(D))  \n",
    "    if not verbose:\n",
    "        iter = range(D)\n",
    "\n",
    "    for d in iter:\n",
    "        di = N_idx[d]\n",
    "        W_ = Wp1[di]\n",
    "        C_one_hot_ = C_one_hot[di]\n",
    "\n",
    "        Nd = len(W_)\n",
    "\n",
    "        ## Stacking W\n",
    "        W_stacked = np.stack(K*[W_], axis=-1)\n",
    "\n",
    "        ## Elementwise product combines logical & in conditio\n",
    "        #  Choosing int32 as product dtype for efficiency.\n",
    "        C_Dot_W = np.multiply(W_stacked, C_one_hot_)\n",
    "\n",
    "        ## The v-dimension of N is a one-hot encoding for the vocabulary:\n",
    "        N_dNdKVp1 = np_one_hot(C_Dot_W.astype(np.int32), V+1)\n",
    "\n",
    "        ## Reverting the v-shift by dropping the 0 one-hot dimension\n",
    "        N_dNdKV = N_dNdKVp1[:, :, 1:]\n",
    "\n",
    "        ## Summing along v-dimension\n",
    "        assert N_dNdKV.shape[0] == Nd\n",
    "        N_dKV = np.sum(N_dNdKV, axis=0)\n",
    "\n",
    "        ## Append to full tensor\n",
    "        N_DKV[d, :, :] = N_dKV\n",
    "\n",
    "    return N_DKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test2 = singleloop_N_tensor(W, C_one_hot, N_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit singleloop_N_tensor(W, C_one_hot, N_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy implementation which avoides all loops but the loop over documents is about a factor 63.2 more efficient as multiloop version.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numpy implementation which avoides all loops but the loop over documents is about a factor {round(8150./129., 1)} more efficient as multiloop version.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fungen_pad_CW(N_idx):\n",
    "    \n",
    "    @tf.function\n",
    "    def padd_CW(W, C_one_hot):\n",
    "        K = C_one_hot.shape[1]\n",
    "\n",
    "        ## Reshaping C to C_DNmaxK and\n",
    "        #            W to W_DNmax\n",
    "        C_DNmaxK_list = []\n",
    "        W_DNmax_list  = []\n",
    "        N_max = tf.reduce_max(N_idx.single_lengths)\n",
    "\n",
    "        for di in N_idx:\n",
    "            CdiK = C_one_hot[di, :]\n",
    "            Wdi  = W[di]\n",
    "            Nd   = CdiK.shape[0]\n",
    "\n",
    "            N_pad = N_max - Nd\n",
    "\n",
    "            ## C\n",
    "            pad = tf.zeros(shape=(N_pad, K), dtype=CdiK.dtype)\n",
    "            CdiK_pad = tf.concat([CdiK, pad], axis=0)\n",
    "            C_DNmaxK_list.append(CdiK_pad)\n",
    "\n",
    "            ## W\n",
    "            pad = tf.zeros(shape=(N_pad), dtype=Wdi.dtype)\n",
    "            Wdi_pad = tf.concat([Wdi, pad], axis=0)\n",
    "            W_DNmax_list.append(Wdi_pad)\n",
    "\n",
    "        C_DNmaxK = tf.stack(C_DNmaxK_list, axis=0)\n",
    "        W_DNmax  = tf.stack(W_DNmax_list, axis=0)\n",
    "\n",
    "        return W_DNmax, C_DNmaxK\n",
    "\n",
    "    return padd_CW\n",
    "\n",
    "## Using a tf.function decorator causes the creation to take pretty long because a graph\n",
    "#  is built which has to take the document lengths into account. This might be only \n",
    "#  useful if the function itself takes long to run without a tf.function... Maybe\n",
    "#  Better use just the plain-non tf.function-converted:\n",
    "\n",
    "def padd_CW(W, C_one_hot, N_idx):\n",
    "    K = C_one_hot.shape[1]\n",
    "\n",
    "    ## Reshaping C to C_DNmaxK and\n",
    "    #            W to W_DNmax\n",
    "    C_DNmaxK_list = []\n",
    "    W_DNmax_list  = []\n",
    "    N_max = tf.reduce_max(N_idx.single_lengths)\n",
    "\n",
    "    for di in N_idx:\n",
    "        CdiK = C_one_hot[di, :]\n",
    "        Wdi  = W[di]\n",
    "        Nd   = CdiK.shape[0]\n",
    "\n",
    "        N_pad = N_max - Nd\n",
    "\n",
    "        ## C\n",
    "        pad = tf.zeros(shape=(N_pad, K), dtype=CdiK.dtype)\n",
    "        CdiK_pad = tf.concat([CdiK, pad], axis=0)\n",
    "        C_DNmaxK_list.append(CdiK_pad)\n",
    "\n",
    "        ## W\n",
    "        pad = tf.zeros(shape=(N_pad), dtype=Wdi.dtype)\n",
    "        Wdi_pad = tf.concat([Wdi, pad], axis=0)\n",
    "        W_DNmax_list.append(Wdi_pad)\n",
    "\n",
    "    C_DNmaxK = tf.stack(C_DNmaxK_list, axis=0)\n",
    "    W_DNmax  = tf.stack(W_DNmax_list, axis=0)\n",
    "\n",
    "    return W_DNmax, C_DNmaxK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit tf_padd_CW(W, C_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit padd_CW(W, C_one_hot, N_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a tf.function the padding is about a factor 15.4 more efficient, when the graph is already constructed. Yet constructing the graph takes a while.\n"
     ]
    }
   ],
   "source": [
    "print(f\"With a tf.function the padding is about a factor {round(970./63., 1)} more efficient, when the graph is already constructed. Yet constructing the graph takes a while.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_DNmax, C_DNmaxK = padd_CW(W, C_one_hot, N_idx)\n",
    "# tf_padd_CW = fungen_pad_CW(N_idx)\n",
    "# W_DNmax, C_DNmaxK = tf_padd_CW(W, C_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_N_tensor(W_DNmax, C_DNmaxK, V=25):\n",
    "\n",
    "    ## Extracting shapes\n",
    "    D = C_DNmaxK.shape[0]\n",
    "    N = C_DNmaxK.shape[1]\n",
    "    K = C_DNmaxK.shape[2]\n",
    "    \n",
    "    ## Preparing W-stacking by shifting all entries one \"up\" s. t. v is counted \n",
    "    #  from 1 to 25 instead from 0 to 24. This enables to collapse the \"&\" in the\n",
    "    #  set to be collapsed to a matrix product\n",
    "    Wp1 = W_DNmax + 1\n",
    "    W_stacked = tf.stack(K*[Wp1], axis=-1)    \n",
    "\n",
    "    ## Elementwise product combines logical & in condition.\n",
    "    #  Choosing int32 as product dtype for efficiency.\n",
    "    C_DNmaxK_int = tf.cast(C_DNmaxK, dtype=tf.int32)\n",
    "    C_Dot_W = tf.math.multiply(W_stacked, C_DNmaxK_int)\n",
    "\n",
    "    ## The v-dimension of N is a one-hot encoding for the vocabulary:\n",
    "    N_DNKVp1 = tf.one_hot(C_Dot_W, V+1, dtype=tf.int32)\n",
    "\n",
    "    ## Reverting the v-shift by dropping the 0 one-hot dimension\n",
    "    N_DNKV = N_DNKVp1[:, :, :, 1:]\n",
    "\n",
    "    ## Summing along v-dimension\n",
    "    assert N_DNKV.shape[1] == N\n",
    "    N_DKV = tf.reduce_sum(N_DNKV, axis=1)\n",
    "\n",
    "    ## Turn to float for gibbs sampler\n",
    "    N_DKV = tf.cast(N_DKV, dtype=tf.float32)\n",
    "\n",
    "    return N_DKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit tf_N_tensor(W_DNmax, C_DNmaxK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The padded tf Version is about a factor 3.5 more efficient than the numpy-vectorized version.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The padded tf Version is about a factor {round(129./37, 1)} more efficient than the numpy-vectorized version.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_test3 = tf_N_tensor(W_DNmax, C_DNmaxK)\n",
    "tf.reduce_all(N_test3 == N_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sampling $C$**\n",
    "\n",
    "Sampling $\\Theta$ and $\\Pi$ is not very problematic because they are just dirichlet distributed (below). Sampling $C$ is sampling from \n",
    "$$\n",
    "    p(C\\vert \\Theta, \\Pi, W)=\\prod_{d=1}^D \\prod_{i=1}^{I_d} \\frac{\\prod_{k=1}^K \\left(\\pi_{dk}\\theta_{kw_{di}}\\right)^{c_{dik}}}{\\sum_{k'=1}^K\\left(\\pi_{dk'}\\theta_{k'w_{di}}\\right)}\n",
    "$$ \n",
    "which is a categorical distribution. The dependence of $I_d$ is somewhat entangled in the $w_{di}$ index of $\\Theta_{kw_{di}}$. A fully vectorized solution without loop over $D$ therefore again needs padding.\n",
    "\n",
    "https://youtu.be/z2q7LhsnWNg?t=3878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Π-Shape\n",
      "(1000, 10)\n",
      "\n",
      "Θ-Shape\n",
      "(10, 25)\n",
      "\n",
      "C-Shape\n",
      "(100174,)\n",
      "\n",
      "C_one_hot-Shape\n",
      "(100174, 10)\n",
      "\n",
      "W-Shape\n",
      "(100174,)\n",
      "\n",
      "C_one_hot_padded-Shape\n",
      "(1000, 135, 10)\n",
      "\n",
      "W_padded-Shape\n",
      "(1000, 135)\n"
     ]
    }
   ],
   "source": [
    "W_DNmax, C_DNmaxK = padd_CW(W, C_one_hot, N_idx)\n",
    "print_all_shapes()\n",
    "print(\"\\nC_one_hot_padded-Shape\")\n",
    "print(C_DNmaxK.shape)\n",
    "print(\"\\nW_padded-Shape\")\n",
    "print(W_DNmax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleloop_sample_C(Theta, Pi, W):\n",
    "    for d in range(D):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_C(Theta, Pi, W_DNmax):\n",
    "\n",
    "    ## Numerator\n",
    "    # Theta_block = tf.cast(tf.stack(D*[Theta]), dtype=tf.float32)\n",
    "    # Pi_block = tf.cast(tf.stack(V*[Pi], axis=-1), dtype=tf.float32)\n",
    "    # numerator = tf.math.multiply(Pi_block, Theta_block)\n",
    "    Theta_KDNmax = tf.gather(Theta, W_DNmax, axis=-1) \n",
    "    Pi_block = tf.stack(Theta_KDNmax.shape[2]*[tf.transpose(Pi)], axis=-1)\n",
    "    numerator = tf.math.multiply(Pi_block, Theta_KDNmax)\n",
    "\n",
    "    ## Denominator\n",
    "    # denominator = tf.reduce_sum(numerator, axis=1)\n",
    "\n",
    "    ## Sampling\n",
    "    C_dist = tfd.Categorical(numerator)\n",
    "    C = C_dist.sample()\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Tets purposes: Calculate Theta_KW in loop once.\n",
    "# Theta_KW_1 = np.zeros(shape=(K, len(W)))\n",
    "\n",
    "# Theta_temp = Theta.numpy()\n",
    "# W_temp = W.numpy()\n",
    "# for k in tqdm(range(K)):\n",
    "#     for i, w in enumerate(W_temp):\n",
    "#         Theta_KW_1[k, i] = Theta[k, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 1000])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO:\n",
    "test = sample_C(Theta, Pi, W_DNmax)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 1000, 135])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta_KDNmax = tf.gather(Theta, W_DNmax, axis=-1) \n",
    "Theta_KDNmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi_block = tf.stack(Theta_KDNmax.shape[2]*[tf.transpose(Pi)], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 1000, 135])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerator = tf.math.multiply(Pi_block, Theta_KDNmax)\n",
    "numerator.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = tf.reduce_sum(numerator, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000, 10, 25])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta_block = tf.cast(tf.stack(D*[Theta]), dtype=tf.float32)\n",
    "Pi_block = tf.cast(tf.stack(V*[Pi], axis=-1), dtype=tf.float32)\n",
    "numerator = tf.math.multiply(Pi_block, Theta_block)\n",
    "numerator.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tensor = tf.cast(Theta[:, W], dtype=tf.float32)\n",
    "# Theta = tf.cast(Theta, dtype=tf.float32)\n",
    "\n",
    "# d = np.random.randint(0, 1000, 1000)\n",
    "# k = np.random.randint(0, 10,   1000)\n",
    "# v = np.random.randint(0, 25,   1000)\n",
    "\n",
    "# for i, j, l in zip(d, k, v):\n",
    "#     test_element = Theta[: W[d, i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sampling $\\Theta$ and $\\Pi$**\n",
    "\n",
    "Sampling $\\Theta$ and $\\Pi$:\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        p(\\Theta\\vert C, W) &= \\prod_{k=1}^K \\mathcal D(\\theta_k; \\ \\beta_{k:} + n_{\\cdot k:}) \\\\\n",
    "        p(\\Pi\\vert C, W)    &= \\prod_{d=1}^D \\mathcal D(\\pi_d; \\ \\alpha_{d:} + n_{d:\\cdot}) \\, .\n",
    "    \\end{align*}\n",
    "$$\n",
    "$\\Theta$ and $\\Pi$ are neither dependent on the number of words per document nor do they have an $I_d$ dimension. Therefore it is much easier to sample them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sample_Theta(N_DKV, beta):\n",
    "    dist_Theta = tfd.Dirichlet(beta + tf.reduce_sum(N_DKV, axis=0))\n",
    "    Theta      = dist_Theta.sample()\n",
    "    return Theta\n",
    "\n",
    "@tf.function\n",
    "def sample_Pi(N_DKV, alpha):\n",
    "    dist_Pi = tfd.Dirichlet(alpha + tf.reduce_sum(N_DKV, axis=-1))\n",
    "    Pi      = dist_Pi.sample()\n",
    "    return Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Initialization of Prior Alpha and Beta.\n",
    "#  They do not acutally matter much but could be optimized by Type II MAP or MLE.\n",
    "beta  = tf.constant(np.random.normal(size=(K, V)), dtype=tf.float32)\n",
    "alpha = tf.constant(np.random.normal(size=(D, K)), dtype=tf.float32)\n",
    "\n",
    "## Calculate one N_DKV for test purposes\n",
    "W_DNmax, C_DNmaxK = padd_CW(W, C_one_hot, N_idx)\n",
    "N_test = tf_N_tensor(W_DNmax, C_DNmaxK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Theta = sample_Theta(N_test, beta)\n",
    "test_Pi    = sample_Pi(N_test, alpha)\n",
    "\n",
    "assert test_Theta.shape == Theta.shape\n",
    "assert test_Pi.shape == Pi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_iter = 200\n",
    "Theta_store = []\n",
    "W_ = W\n",
    "\n",
    "C0         = np.random.randint(0, 10,  size=(D, N))\n",
    "C0_one_hot = tf.one_hot(C0, depth=K, axis=-1)\n",
    "\n",
    "alpha0 = np.random.randint(0, 10,  size=(D, N))\n",
    "beta0  = np.random.normal(size=(D, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for iter in  tqdm(range(N_iter)):\n",
    "#     Ndkv = \n",
    "#     Theta_it = tfd.Dirichlet()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tfa-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b28dcc1f87ebe436630d7ecff3a3c61833e6c22febb81b1e7cd0d34da649d2b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

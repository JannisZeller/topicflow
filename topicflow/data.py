# ------------------------------------------------------------------------------
# topicflow.data
# --------------
# 
# Datasets and samplers to be used with `topicflow.models`.
# ------------------------------------------------------------------------------


# %% Dependencies
# ------------------------------------------------------------------------------
import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp
tfd = tfp.distributions

from collections.abc import Sequence
import matplotlib.pyplot as plt


# %% sliceConverter
# ------------------------------------------------------------------------------
## Convertes slices of given document-lengths to an iterator such that they can 
#  be iterated easily.
class sliceConverter(Sequence):
    def __init__(self, N: np.ndarray):
        assert N.ndim == 1
        N_idx = np.cumsum(N)
        N_idx = np.repeat(N_idx, 2)
        self.N_idx   = np.concatenate([[0], N_idx], axis=0)
        self.N_max   = np.mean(N)
        self.N_total = N_idx[-1].numpy()
        self.single_lengths = N

    def __len__(self):
        return int((self.N_idx.shape[0] - 1) / 2)

    def __getitem__(self, k):
        if type(k) == int:
            if k >= 0:
                return slice(self.N_idx[2*k], self.N_idx[2*k + 1], 1)
            if k < 0:
                i = len(self) + k
                return slice(self.N_idx[2*i], self.N_idx[2*i + 1], 1)
        elif type(k) == slice:
            start, stop, step = k.indices(len(self))
            return [self[i] for i in range(start, stop, step)]
        else:
            raise TypeError("Index must be integer or slice.")

    def __iter__(self):
        self.idx = 0
        self.maxiter = len(self)
        return self

    def __next__(self):
        if self.idx < self.maxiter:
            ret = self.__getitem__(self.idx)
            self.idx += 1
            return ret
        else:
            raise StopIteration

            
# %% Square Documents Model
# ------------------------------------------------------------------------------
## Inspired by Griffiths & Steyver (2004, 
#  https://doi.org/10.1073/pnas.0307752101). Can be used as a visual tool to
#  check models. Creates documents with a vocabulary of the size of int^2, i. e.
#  they can be visualized as square-images.

class squareLDDocuments:
    """Constructs documents generated by a latent dirichlet process which can be
    represented as suqare matrices of counts (vocab size is square).
    """
    def __init__(
        self,
        N_docs:     int, 
        N_topics:   int,
        sqrt_N_vocab:  int,
        N_words_fixed: int=None,
        N_words_rate:  int=None,
        Alphas: float=1.
        ) -> object:
        """
        Parameters
        ----------
        N_docs : int
            Number of documents
        N_topics : int
            Number of latent topics
        sqrt_N_vocab : int
            Size of square root of vocab size
        N_words_fixed : int=None
            Number of words per document fixed.
        N_words_rate : int=None
            Number of words per document as rate of a poisson distribution
        Alphas: float
            Sparsity of Document-Topic matrix (higher is LESS sparse)
        Returns
        -------
        np.ndarray : 
            squareLDDocuments object.
        """
        
        assert (N_words_fixed is None) != (N_words_rate is None), "Exactly one of N_words_fixed and N_words_rate must be passed."

        ## Number of words per document
        if N_words_rate is not None:
            N_D_dist = tfd.Poisson(rate=100)
            N_D      = tf.cast(N_D_dist.sample(N_docs), dtype=tf.int32)
        if N_words_fixed is not None:
            N_D = np.array(N_docs*[N_words_fixed])
        N_idx    = sliceConverter(N_D)
        

        ## Word grid
        N_vocab = int(sqrt_N_vocab**2)
        V_grid = np.reshape(np.arange(0, N_vocab), (sqrt_N_vocab, sqrt_N_vocab))


        ## Topic-Word Distribution
        #  Words belonging to a topic are rows and columns
        Theta_idx = [row for row in V_grid] + [col for col in V_grid.T]
        Theta = np.zeros((N_topics, N_vocab))
        for k, idx in enumerate(Theta_idx):
            Theta[k, idx] = 1. / sqrt_N_vocab
        Theta = tf.constant(Theta, dtype=tf.float32)


        ## Document-Topic Distribution
        #  Initializing Alpha manually to 1.
        Alpha = 1
        dist_Pi = tfd.Dirichlet(N_topics*[Alpha])
        Pi      = dist_Pi.sample(N_docs)


        ## Topic Assignments of word c_{dik} of word w_{di}
        dist_C  = tfd.Categorical(probs=Pi)
        C_NmaxD = dist_C.sample(tf.reduce_max(N_D))

        C_T = []
        for d in range(N_docs):
            C_T_col = C_NmaxD[:N_D[d], d]
            C_T.append(C_T_col)
        C_T = tf.concat(C_T, axis=0)
        C_TK = tf.one_hot(C_T, depth=N_topics, axis=-1)


        ## Draw words w_{di}
        dist_W_T = tfd.Categorical(probs=tf.gather(Theta, C_T))
        W_T      = dist_W_T.sample()


        return Theta, Pi, C_T, C_TK, W_T, N_idx
